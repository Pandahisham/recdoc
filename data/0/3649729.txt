Iron and Malaria Interactions: Programmatic Ways Forward12

Around one-quarter of the world’s children suffer from iron deficiency anemia, and many of them live in malaria-endemic areas. However, there is evidence that iron supplements can increase risk of severe malaria morbidity. The dilemma is how to move forward with interventions to prevent iron deficiency and its consequences in young children, using strategies that minimize risks of malaria and related infections. Screening for iron deficiency is problematic for several reasons. Two complementary strategies are suggested for moving forward with interventions to prevent iron deficiency in children exposed to malaria. The first is to reduce exposure to iron in the form of supplements by: adopting a lifecycle approach to pediatric iron deficiency beginning in utero, using the lowest adequate dose, and giving iron in or with foods. The second is to coordinate iron interventions with malaria control efforts. To stop all iron interventions in malaria-endemic areas is an unreasonable policy option. While research findings continue to increase our understanding, there are also programmatic ways forward with the knowledge at hand.

It is estimated that one-quarter of the world’s children suffer from iron deficiency anemia (1). Iron-deficient children suffer from defects in neurodevelopment, with effects that persist into middle childhood and adolescence (2). Iron deficiency is common, because it is practically impossible to meet the dietary iron needs of rapidly growing young children with the monotonous, largely plant-based diets that are commonly fed to children in many parts of the world (3). Without provision of bioavailable heme iron, iron-fortified foods, or iron supplements, many infants become iron deficient. Therefore, as a preventive intervention, low-dose oral iron supplements have been routinely recommended for infants and young children beyond the age of 6 mo and for low-birth weight infants beginning at 2 mo of age (4).
However, there is evidence that iron supplements can, at least for some children in some circumstances, increase risk of severe malaria morbidity. Evidence of an iron-malaria interaction in the human host is long-standing (5), although the implications for public health programs have been unclear. Most experts have assumed that the tight regulation of iron absorption in the gut could prevent dangerous effects of iron supplements. This assumption was supported by meta-analysis of available randomized clinical trials in 1998, showing no significantly elevated risks of malaria-related outcomes with low-dose oral supplements in children (6). A more recent meta-analysis reached the same conclusion (7).
In 2006, a large, individually randomized, placebo-controlled trial of iron + folic acid supplements reported an elevated risk of hospitalization or mortality in infants who received iron compared to those receiving placebo (8). This result, observed in Pemba Island, Zanzibar, an area of very high P. falciparum malaria transmission and mortality, strongly implicated a malaria connection, because a parallel trial done in rural Nepal, where malaria transmission is low, did not find an adverse effect (9). These were the first randomized trials of iron-containing supplements to be designed with child mortality as primary outcomes, and they carried a great deal of weight in the minds of policy-makers.
In response to the Pemba trial, the WHO and UNICEF issued a statement in 2007 (10) that for all practical purposes, halted progress in iron interventions in many malaria-endemic areas:“Universal iron supplementation (i.e., use of medicinal iron as pills or syrups) should not be implemented without the screening of individuals for iron deficiency, because this mode of iron administration may cause severe adverse events in iron-sufficient children.”Although the statement allowed for iron supplements if targeted to iron-deficient children, no programs were implemented with that strategy because of logistical barriers to screening. The statement also allowed the use of iron-fortified complementary foods, but many programs seeking to utilize iron-fortified foods, micronutrient powders, or lipid-based nutrition supplements for children were also stymied due to concerns about adverse events.
“Universal iron supplementation (i.e., use of medicinal iron as pills or syrups) should not be implemented without the screening of individuals for iron deficiency, because this mode of iron administration may cause severe adverse events in iron-sufficient children.”
But dietary iron deficiency is not a solution to malaria morbidity or mortality. Iron-malaria interactions notwithstanding, children who are iron deficient, as well as those who are iron replete, suffer and die from malaria. Effective prevention and treatment strategies for malaria exist and the current funding for those interventions, although still inadequate, is unprecedented (11). Many malaria control programs are achieving impressive measures of success (12, 13).
The dilemma is how to move forward with interventions to prevent iron deficiency and its consequences in young children, using strategies that minimize risks of malaria and related infections. The dilemma is especially difficult, because our knowledge is imperfect. This is not a unique circumstance. Public health practice is often confronted with problems that are complex, involve uncertain risks and multiple competing objectives, and engage stakeholders with different perspectives (14). To stop all iron interventions in malaria-endemic areas is a policy option. But given the consternation of the international nutrition community (evidenced, e.g., by this symposium), this option appears to be unacceptable to public health communities of practice, malaria as well as nutrition.
In the ideal world, we would target iron interventions only to the children who would benefit from those interventions and therefore not devote any resources to nonbenefitters. In the real world, we are often unable to predict the potential to benefit with high accuracy, and the costs of predicting (i.e., screening or targeting) may exceed the costs of delivering the intervention to nonbenefitters.
In a substudy of the Pemba, Zanzibar trial in which children’s iron status at baseline was characterized, children who were iron deficient had no adverse risks with iron supplementation but in fact benefited greatly, with a highly significant 49% (8) reduction in the risk of hospitalization or mortality. On this basis, the WHO 2007 statement left open the possibility of providing iron supplements after screening for iron deficiency.
This recommendation was problematic in multiple ways. First, the best screening indicator is unknown. Zinc protoporphyrin (measured in whole blood using a hematofluorometer) was the indicator used in the Pemba trial (8), but this methodology is not widely available and the interpretation of values and cutoffs is known to vary with infections and lead exposure. Furthermore, cost-effective delivery systems for targeting have been discussed in theory (15) but not tested in any programmatic setting.
Iron interventions could be targeted to infants diagnosed with iron deficiency (as per the WHO recommendation). But pending better and cheaper technologies [as discussed by Crowley et al. (16) in this supplement], this would be feasible only in rather well-equipped clinical settings. Even if it could be implemented, to target children with known iron deficiency is a treatment strategy, not a prevention strategy. If the effects of iron deficiency on child development are not fully reversible, prevention is paramount (17).
Another recommended form of targeting is to supplement low-birth weight infants, who are known to be at high risk of iron deficiency (4). This long-standing recommendation has never gained widespread global coverage, most probably due to poor documentation of birth weight in many settings. Even if this were widely implemented, many normal-birth weight infants would also develop iron deficiency and would be missed.
In addition, in the past decade, single micronutrient interventions such as iron supplements have given way to a broader agenda to improve infant and young child nutrition. Thus, most current programs seek to employ strategies and products that combine several micronutrients (e.g., micronutrient powders) and, often in combination with some macronutrients, in the form of fortified lipid pastes or processed cereals. Strategies for specifically targeting a single nutrient such as iron become even more impractical within integrated nutrition interventions.
I suggest 2 strategies for moving forward with interventions to prevent iron deficiency in children exposed to malaria. These are not mutually exclusive but rather should be considered in combination.
The most fundamental strategy for reducing risk is to reduce the iron dose. In support of this strategy, programs should adopt a lifecycle approach to pediatric iron deficiency beginning in utero.
Once iron is absorbed into the body, it is efficiently stored and highly conserved through recycling of senescent red cells. Therefore, the iron status of a young child is strongly influenced by its prior status, over a period of weeks and months or even years. The developing fetus accumulates iron from its mother and is born with an iron endowment that is designed to sustain its physiologic needs for several postnatal months. Total body iron (TBI) at birth was estimated in Zimbabwean neonates, and the quartile of TBI at birth strongly predicted the risk of anemia in the infant up to 12 mo postnatally (18). Adequate body iron at birth was a function of both the size of the baby and the iron status of the mother. Strengthening interventions to prevent iron deficiency in pregnant women and low birth weight in their babies will improve neonatal iron status. Delaying the clamping of the umbilical cord further endows the baby with iron in the form of hemoglobin (19). Ensuring that infants are born with adequate iron stores will reduce the need for supplemental iron during infancy and toddler-hood.
Interventions will still be needed, however, to prevent iron deficiency during the period of complementary feeding. To minimize risks, it is reasonable to give iron in or with foods rather than to rely on supplements given without food. Iron ingested in foods is absorbed more slowly, potentially mitigating risks associated with a postprandial rise in nontransferrin-bound iron (20). Strategies include fortifying infant foods such as cereals or adding micronutrient powders or fortified lipid pastes to nonfortified foods.
Furthermore, when iron is given either as a fortificant or as a supplement, it is best to the use the lowest possible adequate dose. Even if iron absorption is well regulated, delivering excess elemental iron to the gut changes the microbiota of the gut in ways that might be detrimental to growth and the developing immune system (21). Recommended doses of supplemental iron have been in the range of 1 RDA, or 12.5 mg (4). However, most babies do not require a full RDA of supplemental iron. Lower dose preparations are currently being tested for efficacy and safety (22).
The second and complementary strategy is to coordinate iron interventions with malaria control efforts. Malaria control programs and infant and young child nutrition programs share the important goal of reducing the prevalence and severity of anemia in children. In the numerous policy-oriented discussions of this dilemma in which I have participated, our colleagues in malaria control are not asking us to stop iron interventions. Instead, they are eager to implement effective malaria control interventions and would be glad for the children to be well nourished in addition.
Coordination of iron interventions with malaria interventions could happen at several levels. At the community level, iron interventions could be prioritized in communities with high coverage of preventive and curative interventions. It is noteworthy that in Pemba, Zanzibar, where the clearest adverse effects of iron-containing supplements were observed, neither curative nor preventive interventions were available to the population, and in the substudy where curative interventions were made available by the investigators, there was no overall elevation of risk with iron + folic acid supplementation (8). Thanks to strengthened global efforts to control malaria, the expanding coverage of malaria interventions is widely measured and publicly documented.
A tighter coordination would be possible if iron interventions were codistributed with key malaria interventions, such as indoor residual spraying, intermittent presumptive treatment of infants, or behavior change communication to caregivers about malaria detection and treatment. If both iron and malaria interventions were made available at the same time and place, programs could be more certain that the children receiving supplemental or fortificant iron also had access to malaria interventions.
It has been 4 y since the WHO and UNICEF published their initial statement in response to the Pemba and Nepal trials of iron + folic acid supplementation. Since that time, a new meta-analysis and systematic review provided a comprehensive analysis of the available evidence from randomized trials (7) and the WHO will soon issue an updated guideline. To stop all iron interventions in malaria-endemic areas is an unreasonable policy option. Although the iron-malaria dilemma remains salient and our knowledge remains imperfect, the nutrition and malaria policy and program communities have already evolved and responded. New strategies are possible that combine safer interventions and stronger integration with malaria control activities. While new research findings continue to increase our understanding, there are also programmatic ways forward with the knowledge at hand.
