Year in review in Intensive Care Medicine 2009. Part III: Mechanical ventilation, acute lung injury and respiratory distress syndrome, pediatrics, ethics, and miscellanea


During 2009 Intensive Care Medicine has published several articles analyzing different aspects of mechanical ventilation and respiratory mechanics, acute lung injury (ALI), and acute respiratory distress syndrome (ARDS).
Several papers have evaluated the respiratory effects of specific maneuvers or drugs on ventilatory pattern. Wu et al. [1] proposed that higher airway occlusion pressure (P0.1) responses to hypercapnic challenge (HC) may indicate less severe injury. Their study aim was to determine whether P0.1 responses to HC were associated with successful weaning after prolonged mechanical ventilation (PMV) in 42 patients with brainstem lesions. Breathing parameters and P0.1 were measured before HC. Three-minute HC challenges with increasing CO2 concentrations were initiated, and P0.1, respiratory rate, minute ventilation (VE), tidal volume (VT), and end-tidal CO2 were measured. Patients were classified into high (group I) and low (group II) response groups on the basis of P0.1 responses to HC. VE and VT increases after HC were significantly greater in group I. P0.1 levels were significantly higher in group I compared with in group II before HC. The increase in P0.1 following HC was significantly greater in group I compared with group II patients. Weaning success was significantly higher in group I compared with group II patients (72.2% versus 33.3%, P = 0.02). The authors concluded that assessing the P.01 response to serial increases of HC levels may be a safe means to ascertain whether patients with brainstem lesions are ready for ventilator weaning.
The respiratory, metabolic, and hemodynamic effects of clonidine in ventilated patients presenting with withdrawal syndrome have been evaluated by Liatsi et al. [2]. After sedation interruption, metabolic oxygen consumption (VO2), CO2 production (VCO2), resting energy expenditure (REE), minute ventilation (VE), tidal volume (VT), respiratory rate (RR), and hemodynamic parameters [heart rate (HR), systolic arterial pressure (SAP), mean arterial pressure (MAP)] were measured in 30 ventilated intensive care unit (ICU) patients. Measurements were performed first under sedation with remifentanil-propofol, then after sedation interruption, and finally after clonidine administration (0.9–1.8 mg clonidine in two doses of 10 min interval). Sedation interruption produced significant increases in SAP and MAP by 33%, HR by 37%, and metabolic rate (increase in VO2 by 70%, VCO2 by 88%, and REE by 74%), leading to high respiratory demands (increase in VE from 9 to 15 l/min). VE was increased due to a twofold increase in RR; VT remained constant. In 25 out of 30 patients, clonidine administration decreased the hemodynamic metabolic and respiratory parameters to values close to those observed with sedation. The authors concluded that, in patients with withdrawal syndrome, clonidine significantly decreased the elevated hemodynamic, metabolic, and respiratory demands occurring after suspension of sedative drugs, inducing mild sedation and facilitating patient cooperation with ventilation and weaning.
The effects of remifentanil-propofol analgosedation on duration of ventilation and length of ICU stay (compared with a conventional regimen) have been evaluated by Rozendaal et al. [3] in 15 Dutch hospitals, showing that, in patients with expected short duration of mechanical ventilation (MV), remifentanil significantly improved sedation and agitation levels and reduced weaning time.
Several study have been dedicated to the clinical use of noninvasive ventilation (NIV) in the ICU setting.
Cosentini et al. [4] assessed the outcome of 454 consecutive acute cardiogenic pulmonary edema (ACPE) patients in an observational, retrospective study. Potential predictors of in-hospital mortality considered of clinical relevance and immediately accessible on admission were investigated by multivariable logistic regression. ACPE-related mortality rate was 3.8% (17/452 patients), and the in-hospital mortality rate was 11.4% (50/440 patients).
Significant independent predictors of increased risk of in-hospital mortality were advanced age (P = 0.012), normal to low blood pressure (P < 0.001), low PaO2/FiO2 ratio (P = 0.020), hypocapnia (P = 0.009), and anemia (P = 0.05).
The predictive factors of NIV failure in critically ill children have been evaluated by Mayordomo-Colunga et al. [5] in a group of 116 patients. Respiratory rate, heart rate, and FiO2 before NIV, and expiratory and support pressures were collected at 1, 6, 12, 24, and 48 h. Factors predicting NIV failure were determined by multivariate analysis. NIV success rate was 84.5%. Higher pediatric risk of mortality (PRISM) score [odds ratio (OR) 1.138; 95% confidence interval (CI), 1.022–1.267] and lower RR decrease at 1 h and at 6 h (OR 0.926; 95% CI, 0.860–0.997 and OR 0.911; 95% CI, 0.837–0.991, respectively) were independently associated with NIV failure.
Use of NIV for acute respiratory failure (ARF) after lung resection has been investigated by Lefebvre et al. [6]. Of the 690 patients at risk of severe complications following lung resection, 113 (16.3%) experienced ARF. NIV was used in 89 (78.7%), including 59 with hypoxemic ARF (66.3%) and 30 with hypercapnic ARF (33.7%). The overall success rate of NIV was 85.3% (76/89). ICU mortality was 6.7% (6/89). The mortality rate following NIV failure was 46.1%. Predictive factors of NIV failure in univariate analysis were age (P = 0.046), previous cardiac comorbidities (P = 0.0075), postoperative pneumonia (P = 0.0016), admission to the surgical ICU (P = 0.034), no initial response to NIV (P < 0.0001), and occurrence of noninfectious complications (P = 0.037).
The role of alternative interfaces for continuous positive airway pressure (CPAP) and NIV has been evaluated by Foti et al. [7], who used helmet CPAP as first-line prehospital treatment of presumed severe acute pulmonary edema. In prehospital treatment of 62 patients with ACPE, CPAP was added to standard medical treatment, while in another 59 patients CPAP was used as sole therapy. Helmet CPAP was feasible in all cases. No patient required prehospital intubation. In both groups, CPAP significantly improved oxygenation, reduced RR, and improved hemodynamics, with more pronounced decrease in blood pressure in the group with combined medical treatment. In the two cohorts, four and five patients were, respectively, intubated in the Emergency Department, and 11 and 9 died.
Bellani et al. [8] described and tested a modified Boussignac system for noninvasive CPAP, aimed at reducing the decrease in FiO2 with higher inspiratory peak flow rates. The authors modified a Boussignac circuit by inserting a T-piece between the Boussignac valve and the face mask. The T-piece was connected to a reservoir balloon receiving oxygen from an independent source. The system was tested in a bench study, consisting of five steps, with increasing inspiratory peak flow rate (Vinsp). Three levels of positive end-expiratory pressure (PEEP) were tested: 7, 10, and 13 cmH2O, with the following devices: Boussignac, Boussignac with reservoir but without supplementary oxygen, and Boussignac with reservoir and 10 (SUPER-Boussignac10) and 30 l/min (SUPER-Boussignac30) supplementary oxygen. In each step, FiO2, VT, and airway pressure were measured. FiO2 increased with PEEP and decreased with increasing Vinsp for all the systems. However, FiO2 increased with SUPER-Boussignac10 (7–10%) and with SUPER-Boussignac30 (10–30%). Moreover, in the latter case, for Vinsp values up to 60 l/min, FiO2 became independent of Vinsp. The SUPER-Boussignac allowed also smaller drop in airway pressure during inspiration and higher tidal volumes. It was concluded that SUPER-Boussignac represents a simple way to significantly improve the performance of the Boussignac device.
Few studies have evaluated mechanisms of respiratory failure in the very early phases of acute exacerbation of chronic obstructive pulmonary disease (COPD). Purro et al. [9] measured respiratory mechanics in patients with early COPD exacerbation treated with NIV or with medical therapy only, and compared data with those obtained in stable, long-term mechanically ventilated, COPD patients [intermittent positive-pressure ventilation (IPPV)]. Patients treated with NIV and IPPV had rapid shallow breathing associated with higher drive to breath and higher diaphragmatic tension-time index than patients managed with medical therapy only, suggesting that, during the early phases of acute exacerbation of COPD, there is a load–capacity imbalance of respiratory muscles similar to that observed in stable COPD. Only dynamic, intrinsic PEEP was different between NIV and IPPV patients, with highest values observed in the latter group.
Patients with chronic respiratory insufficiency often suffer from sleep disruption, secondary to impaired ventilation and gas exchange during sleep. In these patients, Ambrogio et al. [10] evaluated whether a new ventilatory mode, averaged volume-assured pressure support (AVAPS), and lateral position were associated with better sleep efficiency than conventional pressure support (PS) and supine position during NIV. Sleep efficiency was comparable with AVAPS and PS, in spite of greater minute ventilation with AVAPS, and was worse in supine than lateral position. Minute ventilation was not influenced by body position, and decreased progressively from wakefulness through various stages of sleep.
Adaptation to the interface plays an important role in NIV efficacy. In a randomized, controlled trial, Cuvelier et al. [11] compared the clinical efficacy of cephalic and oronasal masks in patients with acute hypercapnic respiratory failure. Respiratory acidosis, encephalopathy and respiratory distress scores, and respiratory rate improved similarly with both masks, indicating comparable clinical efficacy. Tolerance to oronasal mask gradually improved, and it was better than cephalic mask at 24 and 48 h.
Iron lung ventilation (ILV) is an alternative form of noninvasive ventilatory assistance for COPD patients with acute exacerbation. In these patients, Corrado et al. [12] compared the efficacy of ILV versus conventional NIV delivered through face mask. When used as first line, ILV had a success rate greater than NIV (87% versus 68%); after the shift of the techniques, however, need for endotracheal intubation and mortality were similar. Sequential use of both techniques significantly increased the total success rate and avoided endotracheal intubation in a high percentage of patients.
NIV is increasingly used outside the ICU. Cabrini et al. [13] reported data about NIV use outside the ICU managed by a medical emergency team (MET) in an experienced center. Success rate was high (78%), with large variations among different diseases. Complications were few and without clinical consequences. MET workload was high, with an average of three patients simultaneously treated. Demoule [14], in the related editorial, highlighted that application of NIV outside the ICU under the supervision of an MET is an interesting approach that needs, however, high training and expertise and is not easy to apply in every hospital.
Patient–ventilator asynchrony during NIV for ARF has been assessed in a multicenter study by Vignaux et al. [15]. Airway pressure, flow, and surface diaphragmatic electromyography were recorded continuously for 30 min. Asynchrony events and the asynchrony index (AI) were determined from visual inspection of the recordings and clinical observation in 60 patients, 55% of whom were hypercapnic. Autotriggering was present in 8 (13%) patients, double triggering in 9 (15%), ineffective breaths in 8 (13%), premature cycling in 7 (12%), and late cycling in 14 (23%). AI >10%, indicating severe asynchrony, was present in 26 patients (43%), whose median (25–75% interquartile range, IQR) AI was 26% (15–54%). A significant correlation was found between the magnitude of leaks and the number of ineffective breaths and severity of delayed cycling. Multivariate analysis indicated that the level of pressure support and the magnitude of leaks were weakly, although significantly, associated with AI >10%. Patient comfort scale was higher with AI <10%, suggesting that patient–ventilator asynchrony is common in patients receiving NIV for ARF, and leaks play a major role in generating patient–ventilator asynchrony and discomfort. Because no clear recommendation exists concerning humidification during NIV and high-flow CPAP, and few hygrometric data are available, Lellouche et al. [16] measured hygrometry during NIV delivered to healthy subjects with different humidification strategies of: heated humidifier (HH), heat and moisture exchanger (HME), or no humidification (NoH). For each strategy, a turbine and an ICU ventilator were used with different FiO2 settings, with and without leaks. During CPAP, two different HH and NoH were tested. Inspired gases hygrometry was measured, and comfort was assessed. On a bench, they also assessed the impact of ambient air temperature, ventilator temperature, and minute ventilation on HH performances (with NIV settings). During NIV, with NoH, gas humidity was very low when an ICU ventilator was used (5 mgH2O/l), but equivalent to ambient air hygrometry with a turbine ventilator at minimal FiO2 (13 mgH2O/l). HME and HH had comparable performance (25–30 mgH2O/l), but HME’s effectiveness was reduced with leaks (15 mgH2O/l). HH performances were reduced by elevated ambient air and ventilator output temperatures. During CPAP, dry gases (5 mgH2O/l) were less tolerated than humidified gases. Gases humidified at 15 or 30 mgH2O/l were equally tolerated. It was concluded that HH and HME provide gas with the highest water content. Comfort data suggested that levels above 15 mgH2O/l are well tolerated. In favorable conditions, HH and HMEs are capable of providing such values, even in the presence of leaks. This article has an editorial comment [17]. No recommendation exists concerning humidification during oxygen therapy in ICU. Chanques et al. [18] performed a clinical and bench study to assess discomfort and hygrometry with bubble (BH) and HH during high-flow oxygen therapy (HFOT). Mouth and throat dryness were lower, whereas absolute humidity was greater, with HH than BH, suggesting that HH may decrease dryness symptoms through increased humidity delivered to the patient. Ricard et al. [17] remarked that discomfort during oxygen therapy showed wide interpatient variability and its relief was incomplete with both devices, recommending that individual assessment of discomfort should be performed before starting humidification.
In a bench study [19], 13 commercially available, new-generation, ICU ventilators were compared in terms of trigger function, pressurization capacity during pressure-support ventilation (PSV), accuracy of pressure measurements, and expiratory resistance. Four turbine-based ventilators and nine conventional servo-valve compressed-gas ventilators were tested using a two-compartment lung model. Three levels of effort were simulated. Each ventilator was evaluated at four PSV levels (5, 10, 15, and 20 cm H2O), with and without PEEP (5 cmH2O). Trigger function was assessed as the time from effort onset to detectable pressurization. Pressurization capacity was evaluated using the airway pressure–time product, computed as the net area under the pressure–time curve over the first 0.3 s after inspiratory effort onset. Expiratory resistance was evaluated by measuring trapped volume in controlled ventilation. Significant differences were found across the ventilators, with a range of triggering delays from 42 to 88 ms for all conditions averaged (P < 0.001). Under difficult conditions, the triggering delay was longer than 100 ms and the pressurization was poor for five ventilators at PSV5 and three at PSV10, suggesting an inability to unload patient’s effort. On average, turbine-based ventilators performed better than conventional ventilators, which showed no improvement compared with a bench comparison in 2000. It was concluded that technical performance of trigger function, pressurization capacity, and expiratory resistance differs considerably across new-generation ICU ventilators. ICU ventilators seem to have reached a technical ceiling in recent years, and some ventilators still perform inadequately.
Modern ICU ventilators offer sophisticated monitoring and new modes. Their user-friendliness has been poorly evaluated. Vignaux et al. [20] evaluated the time needed by ten physicians experienced in mechanical ventilation, but without prior knowledge of tested ventilators, to perform specific tasks. All physicians’ times were significantly higher than a reference time (by trained physiotherapist familiar with the devices). Heterogeneity existed among machines, but no ventilator was clearly the best. Task failure (>180 s) was 16%, mainly mode and parameter recognition, starting pressure support, and finding NIV mode. Conclusion was that user-friendliness of ventilators should be improved by promoting closer ties between end-users and manufacturers. Richard et al. [21] underlined that, apart from ventilators’ ergonomics, adequate competency is a prerequisite for independently manipulating the ventilator.
Respiratory pressure–volume (PV) curve is now available on several ventilators. Piacentini et al. [22] compared PV curves obtained from a ventilator with a reference technique (CPAP method) in patients with ALI-ARDS. Lower and upper inflection points, and point of deflation maximum curvature showed good correlation between methods, suggesting that the ventilator tool for PV curve tracing is a valid alternative at the bedside.
Weber-Carstens et al. [23] evaluated spontaneous breathing using pumpless extracorporeal lung assist (p-ECLA) in patients with late ARDS and hypercapnia. The median reduction in PaCO2 was 50% following institution of p-ECLA. Extracorporeal CO2 removal enabled significant reduction in tidal volumes (to below 4 ml/kg predicted body weight) and inspiratory plateau pressures. The proportion of assisted spontaneous breathing increased within 24 h of instituting p-ECLA, suggesting that elimination of CO2 by p-ECLA therapy allowed reduction of ventilator-induced shear stress through ventilation with tidal volumes below 4 ml/kg predicted body weight.
Brogan et al. [24] evaluated, in a retrospective manner, clinical and treatment factors for patients recorded in the Extracorporeal Life Support Organization (ELSO) registry and survival of adult extracorporeal membrane oxygenation (ECMO) respiratory failure patients from 1986 to 2006. Data were analyzed separately for the entire time period and the most recent years (2002–2006). Of 1,473 patients, 50% survived to discharge. Median age was 34 years. Most patients (78%) were supported with venovenous ECMO. In a multivariate logistic regression model, pre-ECMO factors including increasing age, decreased weight, days on mechanical ventilation before ECMO, arterial blood pH ≤7.18, and Hispanic and Asian race compared with White race were associated with increased odds of death. For the most recent years (n = 600), age and PaCO2 ≥70 compared with PaCO2 ≤44 were also associated with increased odds of death. The two diagnostic categories ARF and asthma compared with ARDS were associated with decreased odds of mortality, as was venovenous compared with venoarterial mode. Cardiopulmonary resuscitation (CPR) and complications while on ECMO including circuit rupture, central nervous system infarction or hemorrhage, gastrointestinal or pulmonary hemorrhage, and arterial blood pH <7.2 or >7.6 were associated with increased odds of death. It was concluded that survival among this cohort of adults with severe respiratory failure supported with ECMO was 50%. Advanced patient age, increased pre-ECMO ventilation duration, diagnosis category, and complications while on ECMO were associated with mortality.
Sleep disruption is well recognized in the ICU. Poor sleep quality continues following discharge from hospital in several patients and becomes a chronic disorder in some. Lee et al. [25] aimed to describe the etiology of chronic sleep complaints in survivors of ARDS. Seven ARDS survivors with no previous sleep complaints who reported difficulty sleeping 6 months or more following discharge from hospital were evaluated. Sleep quality was assessed subjectively with a sleep history and the Insomnia Severity Index, and objectively with polysomnography. Daytime sleepiness was assessed with the Epworth Sleepiness Scale. A chronic sleep disorder was identified in each patient who reported difficulty sleeping. The primary sleep disorder was chronic conditioned insomnia (five patients), parasomnia (one patient), and obstructive sleep apnea (one patient). In addition, four patients had periodic leg movements, which was of uncertain clinical significance. It was concluded that survivors of ARDS are at risk to develop chronic sleep disorders, and that further research is required to determine the prevalence and natural history of chronic sleep disorders in this patient population.
An epidemiological study by Metnitz et al. [26] evaluated the current practice of mechanical ventilation in the ICU and the characteristics and outcomes of ventilated patients. The investigation was designed as a preplanned substudy of a multicenter, multinational cohort study of the SAPS 3 project. A total of 13,322 patients admitted to 299 intensive care units (ICUs) from 35 countries were analyzed. More than half of the patients (53%) were mechanically ventilated at ICU admission. FiO2, VT, and PEEP used during invasive MV were on average 50%, 8 mL/kg actual body weight, and 5 cmH2O, respectively. As much as 17.3% of patients under invasive MV were ventilated with zero PEEP (ZEEP). These patients exhibited significantly increased risk-adjusted hospital mortality, compared with patients ventilated with higher PEEP (1.12, 95% CI: 1.05–1.18). Noninvasive ventilation was used in 4.2% of all patients and was associated with improved risk-adjusted outcome (OR 0.79, 95% CI: 0.69–0.90). The authors concluded that mechanical ventilation is one of the most common interventions employed in the intensive care unit (more than 50% of patients were under mechanical ventilation). The use of MV is variable, with 30% of patients having ALI being ventilated with tidal volumes >8 ml/kg actual body weight. Invasive MV with ZEEP was associated with worse outcome, even after controlling for severity of disease. Since this study did not document indications for MV, the association between MV settings and outcome must be viewed with caution. This article has an editorial comment [27].
Linko et al. [28] analyzed ARF incidence, treatment, and mortality in Finnish ICUs. Ventilatory support >6 h was needed in 39% of admitted patients. Incidence of ARF, ALI, and ARDS was 149.5, 10.6, and 5.0/100,000 per year, with ARF 90-day mortality of 31%. Assisted ventilatory modes were preferred (81%). Median tidal volume/predicted body weight was 8.7 ml/kg, and plateau pressure was 19 cmH2O. Thus, while incidence of ARF requiring ventilatory support is higher, incidence of ALI and ARDS is lower in Finland than in other countries. Tidal volumes were higher than recommended for lung protection, but airway pressures were limited in the majority of patients. Estenssoro [29] underlined differences and similarities between this study and others previously published.
Protti et al. [30] wished to clarify whether the gas exchange response to prone position is associated with lung recruitability in mechanically ventilated patients with acute respiratory failure. In 32 patients, gas exchange response to prone position was investigated as a function of lung recruitability, measured by computed tomography in supine position. No relationship was found between increased oxygenation in prone position and lung recruitability. In contrast, the decrease of PaCO2 was related with lung recruitability (R2 = 0.19; P = 0.01). Patients who decreased their PaCO2 more than the median value (−0.9 mmHg) had greater lung recruitability (19 ± 16% versus 8 ± 6%; P = 0.02), higher baseline PaCO2 (48 ± 8 versus 41 ± 11 mmHg; P = 0.07), heavier lungs (1,968 ± 829 versus 1,521 ± 342 g; P = 0.06), and more nonaerated tissue (1,009 ± 704 versus 536 ± 188 g; P = 0.02) than those who did not. It was concluded that, during prone position, changes in PaCO2, but not in oxygenation, are associated with lung recruitability, which in turn is associated with severity of lung injury.
Keulenaer et al. [31] described what is defined as normal intra-abdominal pressure (IAP) and how body positioning, body mass index (BMI), and positive end-expiratory pressure (PEEP) affect IAP monitoring. The underlying notion is that the abdomen truly behaves as a hydraulic system. A review of different databases was made. The definitions of normal IAP in the general patient population and morbidly obese patients were reviewed. Subsequently, factors that affect the accuracy of IAP monitoring, i.e., body position (head-of-bed elevation, lateral decubitus and prone position) and PEEP, were explored. It was concluded that the abdomen behaves as a hydraulic system with normal IAP of about 5–7 mmHg, and with higher baseline levels in morbidly obese patients of about 9–14 mmHg. Measuring IAP via the bladder in the supine position is still the accepted standard method, but in patients in the semirecumbent position (head of bed elevated to 30° and 45°), the IAP on average is 4 and 9 mmHg, respectively, higher. Small increases in IAP in stable patients without intra abdominal hypertension (IAH), turned prone, have no detrimental effects.
Feihl and Broccard [32, 33] provide a Guytonian analysis of the effects of PEEP on cardiac output, and terminate the review with some remarks on the potential of positive pressure breathing to induce acute cor pulmonale, and on the cardiovascular mechanisms that underlie failure to wean a patient from the ventilator. These two articles must be mandatory to read and understand for every in-training intensivist and also for board-certified specialists in this field, providing in-depth, state-of-the-art description of cardiorespiratory interactions. In the first part of the review, the authors provide a few, clear, simple physiological concepts covering the cardiorespiratory interactions to explain the theoretical framework. In the second part of the review, the authors apply these concepts to clinical situations and thus put this theoretical framework of cardiorespiratory interactions into practical use. In particular, they describe mechanisms underlying Kussmaul’s sign and pulsus paradoxus. They review the literature on the use of respiratory variations of blood pressure to evaluate volume status (for instance, by investigating with ultrasonography how the geometry of great veins fluctuates with respiration).
Studies have shown that biomarkers in body fluids of patients with ALI and ARDS may provide insight into the pathogenesis and prognosis of these entities. Calfee et al. [34] aimed to determine whether levels of soluble intercellular adhesion molecule-1 (sICAM-1), a marker of alveolar epithelial and endothelial injury, differ in patients with hydrostatic pulmonary edema and ALI and are associated with clinical outcomes. Measurement of sICAM-1 levels were performed in: (1) plasma and edema fluid from 67 patients with either hydrostatic pulmonary edema or ALI enrolled in an observational, prospective single-center study, and (2) in plasma from 778 patients with ALI enrolled in a large multicenter randomized controlled trial of ventilator strategy. In the single-center study, levels of sICAM-1 were significantly higher in both edema fluid and plasma (median 938 and 545 ng/ml, respectively) from ALI compared with hydrostatic edema patients (median 384 and 177 ng/ml; P < 0.03 for both comparisons). In the multicenter study, higher plasma sICAM-1 levels were associated with poor clinical outcomes. Subjects with ALI whose plasma sICAM-1 levels increased over the first 3 days of the study had higher risk of death, after adjusting for other important predictors of outcome (odds ratio 1.48; 95% CI 1.03–2.12, P = 0.03). The authors concluded that higher plasma sICAM-1 levels and increasing sICAM-1 levels over time are associated with poor clinical outcomes in ALI. Measurement of sICAM-1 levels may be useful for identifying patients at highest risk of poor outcomes from ALI.
Heme oxygenase-1 (HO-1) acts in cytoprotection against acute lung injury, and the polymorphic (GT)n repeat in the HO-1 gene (HMOX1) promoter regulates HMOX1 expression. Sheu et al. [35] investigated the associations of HMOX1 polymorphisms with acute respiratory distress syndrome (ARDS) risk and plasma HO-1 levels in an unmatched, nested case–control study. Consecutive patients with ARDS risk factors upon ICU admission were prospectively enrolled. Cases were 437 Caucasians who developed ARDS, and controls were 1,014 Caucasians who did not. The (GT)n polymorphism and three tagging single-nucleotide polymorphisms (tSNPs) were genotyped in 1,451 patients, and plasma HO-1 levels were measured in 106 ARDS patients. The (GT)n repeats were clustered into: S-allele (<24 repeats), M-allele (24–30 repeats), and L-allele (≥31 repeats). It was found that longer (GT)n repeats were associated with reduced ARDS risk (Ptrend = 0.004 for both alleles and genotypes), but no individual tSNP was associated with ARDS risk. HMOX1 haplotypes were significantly associated with ARDS risk (global test, P = 0.016), and the haplotype S-TAG was associated with increased ARDS risk (odds ratio, 1.75; 95% CI: 1.15–2.68, P = 0.010). Intermediate-phenotype analysis showed that longer (GT)n repeats were associated with higher plasma HO-1 levels (Ptrend = 0.019 for alleles and 0.027 for genotypes). It was concluded that longer (GT)n repeats in the HMOX1 promoter are associated with higher plasma HO-1 levels and reduced ARDS risk. The common haplotype S-TAG was associated with increased ARDS risk. The results suggest that HMOX1 variation may modulate ARDS risk through promoter microsatellite polymorphism. This article has an editorial comment [36].
Preclinical studies suggest that 3-hydroxy-3-methylglutaryl-coenzyme A (HMG-CoA) reductase inhibitors (statins) may attenuate organ dysfunction. In a retrospective study [37], it was evaluated whether statins are associated with attenuation of lung injury and prevention of associated organ failure in patients with ALI/ARDS. From a database of patients with ALI/ARDS, the presence and timing of statin administration were determined. Main outcome measures were development and progression of pulmonary and nonpulmonary organ failure as assessed by changes in PaO2/FiO2 ratio and Sequential Organ Failure Assessment (SOFA) score between days 1 and 7 after the onset of ALI/ARDS. Secondary outcomes included ventilator-free days, ICU and hospital mortality, and lengths of ICU and hospital stay. From 178 patients with ALI/ARDS, 45 (25%) received statin therapy. From day 1 to day 7, the statin group showed less improvement in their PaO2/FiO2 ratio (27 versus 55, P = 0.042). Ventilator-free days (median 21 versus 16 days, P = 0.158), development or progression of organ failures (median ∆SOFA: 1 versus 2, P = 0.275), ICU mortality (20% versus 23%, P = 0.643), and hospital mortality (27% versus 37%, P = 0.207) were not significantly different in the statin and nonstatin groups. After adjustment for baseline characteristics and propensity for statin administration, there were no differences in ICU or hospital lengths of stay. It was concluded that statin use was not associated with improved outcome in patients with ALI/ARDS, and the authors were unable to find evidence for protection against pulmonary or nonpulmonary organ dysfunction. This article was followed by a letter to the editor and its rebuttal [38].
Early detection of ALI may be considered essential for implementation of adequate therapeutic measures. Computerized syndrome surveillance systems may be a useful tool in achieving improved early detection. ALI electronic alert (ALI “sniffer”) was developed and validated in a study including 3,795 critically ill patients in the USA [39]. As compared with routine clinical bedside assessment achieving diagnosis of ALI in only 27% of detections, the ALI sniffer improved recognition of ALI significantly. Demonstrated sensitivity of 96% and specificity of 98% by applying the ALI sniffer suggest a role for this approach in daily practice.
Electrical impedance tomography (EIT) is a new tool for bedside monitoring of lung volumes. Bikker et al. [40] evaluated the relationship between measured end-expiratory lung volume (EELV) and EELV calculated by EIT recorded at the basal part of the lung with different PEEP. The tidal impedance variation to tidal volume ratio varied with PEEP and between patients. A low correlation and moderate agreement between methods were observed, suggesting that EIT is inaccurate for calculation of EELV when impedance is measured at only one thoracic level just above the diaphragm during PEEP trial.
EIT in the respiratory field is, however, developing fast, and another couple of papers have focused on this technique. Costa et al. [41] made comparisons between EIT, using a novel algorithm, and computed tomography (CT) for detecting alveolar recruitment. They also went one step further and proposed a measure of lung hyperdistension by combining EIT findings of aeration and regional lung mechanics. Zhao et al. [42] developed and tested an indicator of inhomogeneous ventilation (GI index). They compared 40 patients with lung injury requiring ventilator treatment with 10 patients who were lung-healthy and ventilated during anesthesia. The conclusion was that the GI index is able to detect and quantify gas distribution inhomogeneity, providing a single number that may facilitate trend analysis. This is reminiscent of the various indices that were developed during the 1950s and 1960s from multiple breath nitrogen washout techniques. The papers on alveolar recruitment and ventilation distribution by Costa and Zhao with coworkers bring us to an invited review by Glenny [43] on “Determinants of regional ventilation of blood flow in the lungs.” Glenny covers the different causes of ventilation and perfusion inhomogeneity. It should be remembered that this excellent review is written by a scientist who dared to question the influence on the ventilation and perfusion distributions solely by gravity, and he has successfully demonstrated additional and sometimes more important causes of this inhomogeneity.
A physiological report [44] approached another timely issue—stress and strain—asking “Is there an optimal breath pattern to minimize stress and strain during mechanical ventilation?”. For that purpose they defined an index to quantify the impact of positive pressure ventilation on the lung [the Stress–Strain Index (SSI)] and tested it in a mathematical model. They found that, if the ventilator is adjusted according to this index, the resulting tidal volume would be lower (0.25 l) and the respiratory rate higher (39 breaths/min) than according to the ARDSNet trial (tidal volume 0.42 l and 17 breaths/min).
Some papers deal with a rather recent technique on guiding the ventilator: neurally adjusted ventilatory assist (NAVA). This technique demands that the diaphragm electromyograph (EMG) be measured with an acceptable signal-to-noise ratio. Barwing et al. [45] tested the efficiency of positioning the esophageal EMG catheter according to the following criteria: stable EMG signal, electrical activity mainly in central leads of the catheter, and absence of the p-wave coming from the electrocardiogram (ECG) (OPT technique). Using these criteria all 26 patients that were studied could be ventilated using NAVA. A previously used technique which takes into account the distance from nose to earlobe to xyphoid process was inferior to the OPT technique.
NAVA and proportional-assist ventilation with load-adjustable gain factors (PAV+) were evaluated in two other papers [46, 47]. Brander et al. [46] assessed the degree of lung protection with NAVA as compared with conventional volume-controlled ventilation in a rabbit model of ALI. Tidal volume (VT) was lower and respiratory rate was higher during NAVA. Both oxygenation and arterial CO2 were higher with NAVA. Variables assessing ventilator-induced lung injury (VILI) and nonpulmonary organ dysfunction were similarly decreased with both strategies, compared with high-VT (15 ml/kg) ventilation. The authors concluded that allowing animals with lung injury to choose their respiratory pattern with NAVA is as effective as conventional, volume-controlled ventilation in preventing VILI, in attenuating systemic and extrapulmonary organ inflammation, and in preserving cardiac and kidney function. Brochard, in the related editorial [48], explained the potential relevance of letting patients freely choose their respiratory pattern, underlying that the degree of freedom and the optimal NAVA setting remain matters of research.
Xirouchaki et al. [47] evaluated PAV+ user-friendliness, by comparing the number of changes in ventilator settings and in dosage of analgesics, sedatives, and vasoactive drugs with PAV+ or pressure support (PS). Changes in ventilator settings and in the dose of sedatives were less with PAV+ than with PS. Dyssynchrony as a cause of modification of ventilator settings was more common with PS (41% versus 3%). Conclusion was that PAV+ is a user-friendly mode, associated with fewer changes in ventilator settings and in sedative dosing compared with PS.
Although touching a quite different field of intensive care medicine, a review by Ochoa et al. [49] is very interesting and possibly a bit nearer to the daily practice of the intensivist. The investigators evaluated the diagnostic accuracy of the so-called cuff-leak test for diagnosis of upper airway obstruction secondary to laryngeal edema and for intubation secondary to upper airway obstruction. They selected nine comparable studies in the literature with significant heterogeneity, as expected. Nonetheless, sensitivity and specificity of the test to demonstrate obstruction were quite impressive; the diagnostic odds ratio was significant for both questions. Hence, the authors concluded that, when it is positive (i.e., no “leak”), this test should alert the clinician to a high risk of upper airway obstruction. On the other hand, the detection of a leak (i.e., negative test) has only low predictive value and does not rule out the occurrence of obstruction or the need for re-intubation. This is an important paper especially for units with a high rate of patients with complex neck and/or larynx surgery. A similar problem, i.e., prevention of extubation failure, was the topic of another literature review by McCaffrey et al. [50]. The specific question addressed was whether corticosteroids reduce the rate of extubation failure in intensive care patients. Fourteen randomized trials were selected and demonstrated a reduction in reintubation with use of corticosteroids by roughly 50%. Importantly, this improvement was shown in all age groups of patients. As extubation failure is associated with significant morbidity and mortality, clinicians should consider use of corticosteroids in patients at high risk of extubation failure, similar to in the aforementioned paper.
The extubation outcome is importantly conditioned by cough strength. Beuret et al. [51] evaluated the role of peak cough expiratory flow (PCEF) as a predictor of extubation outcome in patients having successfully passed the spontaneous breathing trial. PCEF was significantly associated with extubation failure. The inability to cough at order or PCEF ≤35 l/min predicted extubation failure with sensitivity of 79% and specificity of 71%.
Problems related to sedative and analgesic drugs are common in both adult and children receiving intensive care. New approaches to the achievement of analgesia and sedation such as protocol-based prescription, daily drug “holidays,” and the use of pain and sedation scores are as relevant in children as in adults. Inoue et al. [52] reported a retrospective cohort study to evaluate whether use of early ketorolac to manage pain increased the risk of renal dysfunction in children undergoing low-risk cardiac surgery. No differences were seen between the two groups in plasma creatinine. The use of ketorolac was associated with an opioid-sparing effect, although no clinically significant difference in length of intensive care stay was evident. The use of scoring systems is well established for titration of analgesia and sedation. Some children exhibit withdrawal symptoms after receiving prolonged or high-dose “sedative” drugs in ICU. However, the occurrence of withdrawal is likely to be underreported, especially if it is not systematically sought. Ista et al. [53] reported a careful observation of a large panel of signs and symptoms including agitation, motor disturbances diarrhea, fever, anxiety, sleep disturbance, and hypertension on 76 children who had received intravenous midazolam and/or opioids in a pediatric ICU (PICU) for >5 days. From this rich resource, 15 discriminatory symptoms were selected by multidimensional scaling for inclusion in a resulting instrument, the Sophia Observation Withdrawal Symptom (SOS) scale. Routine use of withdrawal scores is increasingly recommended for children receiving “sedation” for over 72 h [54]. It is well documented that adult [55] and child [56] survivors of critical illness may exhibit symptoms of depression or other psychological disturbances. Colville et al. [57] focus on a surprisingly positive aspect of postcritical outcome in children, reporting that “posttraumatic growth,” a phenomenon whereby a traumatized person may ultimately come to function at a higher level than before, is detectable in survivors of pediatric critical illness, a finding which has been detected in some other “stress and trauma” survivors [58].
There is currently a high level of interest in both adult and pediatric critical care in the accurate reporting of renal dysfunction. To this end, the risk of renal failure, injury to kidney, failure of kidney function, loss of kidney function and end-stage renal failure (RIFLE) score [59], and its pediatric modifications such as pRIFLE [60], are useful tools. Palmieri et al. [61] used the modified pediatric RIFLE criteria to characterize severity of acute kidney injury (AKI) in a cohort of 123 children with burns, in whom they found an incidence of AKI of 45.5%. Presence of sepsis and septic shock were independent risk factors for the Failure class of AKI. Renal replacement therapy (RRT) in infants and children can be technically very challenging. In the April issue of the Journal, Maclaren and Butt present a succinct review of the subject of pediatric RRT and welcome the discipline and potential for systematic study enabled by the founding of the Pediatric Renal Replacement Therapy Registry [62]. In the same issue of the Journal, Zappitelli et al. report the results of a prospective observational study on the nutrient balance of critically ill children receiving continuous venovenous hemodialysis (CVVHD), specifically dialysis clearance, and quantify amino acid, trace metal, and folate losses, and the relationship of these losses to daily nutrient intake. They conclude that significant amino acid, folate, and some trace metal losses occur by CVHHD, and that these losses may have a significant impact on nutrition provision.
Critical status at all ages is virtually synonymous with mechanical ventilation. Respiratory support in the newborn, especially the premature neonate, is particularly challenging. For this reason, much of our understanding of neonatal lung disease and its management relies on animal studies. Pillow et al. [63] reported a study in the September issue of the Journal in which they studied the inflammatory potential of hyperoxia and low inspired gas humidity in near-term lambs. Compared with 100% oxygen, cold dry gas had a less marked effect on interleukin (IL)-1, IL-8, and cytokine messenger RNA (mRNA) expression, emphasizing the toxic potential of relatively short (3 h) exposure to high FiO2 in the immature lung. Two papers published in 2009 addressed the challenges of respiratory monitoring in neonates. Fisher et al. [64] investigated the accuracy of a commercially available Y-piece flow-sensor-based device [65] to measure tidal volume in preterm neonates receiving nasopharyngeal CPAP. Some measurements were possible, but the investigators found that the %leak often exceeded the measurement range of the device, and that overall reliable measurement was not possible using the system tested. In the same issue of the Journal, Lopez [66] reported a clinical study comparing the use of side-stream end-tidal (EtCO2) and transcutaneous carbon dioxide (TcPCO2) measurement against a reference standard of mixed venous blood (PvCO2) in ventilated preterm infants. Good correlation was observed between TcPCO2 and PvCO2, but not between EtCO2 and PvCO2 or between EtCO2 and TcPCO2. However, EtCO2 accurately identified clinically relevant, high and low PvCO2 cutoffs and could therefore be used as a monitoring device to maintain PvCO2 within safety margins. The importance of performing studies in specific young populations was highlighted by the reports of Riedel et al. and van Veenendaal et al. [67, 68]. Both groups used electrical impedance technology-based measurement techniques. Riedel et al. determined that there were distinct differences between ventilation distribution between preterm and term infants detectable by EIT but not by multiple breath washout (MBW) technique. This is an important finding, as previous studies in neonates based on MBW measurements may need to be reassessed. van Veenendaal et al. report a simple finding, again based on lung volume studies with EIT, that lung volume decreases when episodes of closed endotracheal suction take place in infants receiving high-frequency open-lung ventilation for neonatal respiratory distress syndrome. EIT offers the possibility of noninvasive assessment of lung volume in clinical situations previous regarded as too difficult to study with alternative, invasive techniques. Accurate measurement during ventilation in older children can also be challenging for the clinician. Bikker et al. [69] reported on the measurement of EELV in intubated children without interruption of mechanical ventilation with a ventilator incorporating a built-in nitrogen washout/wash-in EELV capability [70] previously validated in adults. The authors conclude that the nitrogen washout EELV device could measure EELV with good precision in children and that the device can readily be used in mechanically ventilated pediatric patients without need for additional tracer gas or interruption of mechanical ventilation.
Differing modes of ventilatory support continue to compete for places in the pediatric intensive care league tables. Khemani et al. [71] reported a large single-center study of ventilatory strategies in a cohort of 398 mechanically ventilated children with PaO2/FiO2 ratio <300. All were ventilated using pressure control ventilation, with 85% achieving tidal volume less than 10 ml/kg. From their results, the investigators reported that they could not detect any difference in mortality between tidal volumes of 6 and 10 ml/kg and importantly that higher tidal volumes within this range were associated with fewer “ventilator-free days.” Two papers in 2009 reported on use of NIV in critically ill children. Piastra et al. [72] reported their early experience of use of NIV in 23 immunocompromised children with ARDS. Thirteen (55%) avoided intubation. Whilst use of NIV is stated to have been well tolerated and may have some theoretical advantages over invasive ventilation in this particularly vulnerable group, a carefully designed randomized controlled trial is needed to establish the role of NIV in the management of pediatric acute respiratory failure. Paiva et al. reported a study in which they observed that 21/50 (42%) children receiving long-term NIV developed nocturnal hypercapnia. They performed a validation of combined SpO2 and PtcCO2 against capillary arterialized blood gas samples and recommended that such monitors be used in long-term ventilated children. Finally, Stucki et al. [73] reported on use of NIV to support six post-cardiac-surgery children who developed respiratory failure after extubation. The study demonstrated the occurrence of a consistent improvement in the breathing pattern of infants submitted to NIV, as documented by reduction of dyspnea score. Analysis of esophageal pressure curves showed a concomitant significant reduction of respiratory rate, esophageal inspiratory pressure swing (dPes), and esophageal pressure–time product (PTPes) with the onset of NIV, indicating off-loading of respiratory muscles.
The developing lung survives in a hostile environment. Research into aspects of lung development and the pathophysiology of neonatal lung disease continues to thrive. In an in vitro study, De Luca et al. [74] studied the activity of secretory phospholipase A2 (sPLA2) in bronchoalveolar lavage fluid obtained from neonates and infants with healthy lungs to which were added varying concentrations of bile acids. They found that high concentrations of bile acids increased sPLA2 activity, leading to increased surfactant catabolism. The authors conclude that aspiration of fluid containing bile acids may contribute to the development of lung injury. Vento et al. [75] also studied peptidomic profiles in bronchoalveolar lavage fluid obtained on day 3 of life from premature (≤32 weeks gestation) infants representing two groups, those with and those without bronchopulmonary dysplasia (BPD). Six peptides, found to be albumin fragments, were found in significantly higher levels in the BPD group than in control patients. Levels of the matrix metalloproteinase-3 (MMP-3) enzyme were also found at significantly higher concentrations in BPD patients. This is an exploratory study and does not lead to any specific mechanistic conclusion, however these authors have shown that peptidomic techniques can be effectively applied to the study of proteolytic enzymes from small babies. Finally, to conclude the neonatal–surfactant theme, Janssen et al. [76] studied endogenous surfactant metabolism in neonates with severe respiratory failure as a result of congenital diaphragmatic hernia (CDH) and control neonates without significant lung disease. The rate of incorporation of a stable carbon-13 isotope into phosphatidylcholine (PC) was measured from tracheal aspirates, and PC levels measured from epithelial lining fluid. The results showed that infants with severe respiratory failure associated with CDH have decreased surfactant PC synthesis, and the authors speculate that this may be linked to the pathogenesis of severe pulmonary insufficiency.
One paper in particular was enthusiastically welcomed by the pediatric editorial team. Sun et al. [77] from Shanghai, China, submitted an interesting descriptive study on the epidemiology and reported outcomes of ARDS (1994 American–European definitions) in children treated in 25 pediatric ICUs in the Peoples Republic of China. Of 12,018 admissions analyzed, 1.44% developed ARDS, of which 61% died. The authors conclude that different management of pneumonia and sepsis, and ventilatory strategies will hopefully lead to reduction of this relatively high case fatality rate. The importance of the paper for the Journal, however, is not so much the clinical report itself, but the joining together through publication of the pediatric intensive care communities of the West and the East, and the editorial board very much hope that there will be further opportunities to publish high-quality work from Asia and other areas of the developing world.
Cardiovascular drugs, new and old, continue to generate interest in pediatric critical care. Baldasso et al. [78] presented a randomized pilot study of low-dose vasopressin in the support of nonseptic critically ill children with sedation-related hypotension. Use of vasopressin was associated with marked falls in serum sodium concentration and urine output. Although systemic arterial blood pressure rose significantly in the vasopressin children (n = 12) compared with saline control (n = 12), use of vasopressin did not avoid the need for treatment with additional vasoactive agents. It is difficult to be enthusiastic about the further exploration of prophylactic vasopressin in nonseptic children given the high incidence of important side-effects and lack of clear benefit. Unknown effects of a commonly used drug were also revealed in the study of Wong et al. [79], who investigated the influence of dopamine (DOPA) on the relationship between cerebral perfusion and metabolism in preterm infants treated with dopamine using near-infrared spectroscopic measurements. Cerebral fractional oxygen extraction (CFOE) and cerebral blood flow (CBF) showed strong negative correlation in the control infants, but not in the DOPA group. CSvO2 was lower at decreased CBF in the control infants, but not in the DOPA group. These results suggest a previously unrecognized role of exogenous dopamine in preterm infants who, uniquely, have an absence of flow–metabolism coupling together with an immature blood–brain barrier (BBB) that allows dopamine to enter the brain parenchyma, thus restoring flow–metabolism coupling.
Three papers addressed aspects of pediatric cardiac intensive care. Lo et al. [80] reported on the performance of a postoperative cardiac resuscitation simulation training scheme specifically tailored to encompass scenarios and skills required in the pediatric cardiac ICU. High-fidelity simulators are increasingly recognized as valuable tools in the training of individuals and medical teams [81] borrowing from experience of safety training and crew resource management from aviation and similar high-resilience industries. Breuer et al. [82] investigated the potential role of acid-stimulating factors such as gastrin and l-type amino acids in the pathogenesis of stress ulcers in children following cardiac surgery. The most potent acid-stimulating factor, gastrin, showed significant elevation in all patients undergoing open heart surgery. Histidine, the most potent acid-stimulating amino acid, was elevated only in surgery including cardiac arrest. Interesting as these findings are, there are a large number of other known factors, including gastric and specifically mucosal blood flow oxygenation, which were not assessed in this study but which are likely to be clinically relevant to the onset of gastric mucosal erosions and ulceration. Jackman et al. [83] also looked at the phenomenon, late-onset hyperlactatemia (LOH) in children 2–16 years of age following cardiac surgery. They followed arterial blood gas, and plasma lactate and glucose for up to 12 h postoperatively. All patients studied had initial plasma lactate values <3 mmol/l. The findings of this study suggest that late rise in blood lactate concentration is relatively common, occurring in 44% of 44 children undergoing Fontan operations and in 25% of those undergoing other types of cardiac surgery. The investigators could detect no relationship between occurrence of LOH and clinical outcomes including clinical diagnosis of low cardiac output, inotrope score, or renal function. The strong association observed between LOH and hyperglycemia is in keeping with the findings of Maillet [84] from an adult population.
Tissue oxygenation is more easily measured as spectroscopic techniques evolve. Wong et al. [85] evaluated the simultaneous measurements of CBF (Doppler ultrasound) and spectroscopically acquired tissue oxygenation index (TOI) in newborn lambs (n = 8). This study demonstrated TOI to be concordant with changes in CBF during induced mild to moderate hypotension. This concordance supports clinical use of TOI in studying fluctuations in cerebral hemodynamics with physiological changes and clinical interventions in preterm infants, when arterial oxygen saturation and cerebral oxygen consumption remain constant during the measurement. Lee et al. [86] studied the utility of using co-administration of N-acetyl cysteine (NAC) (a scavenger of reactive oxygen species) and N(G)-monomethyl-l-arginine (a nitric oxide synthase inhibitor) in achieving hemodynamic recovery following hypoxia-induced myocardial injury. Whilst this study confirmed the benefit previously shown for NAC, no additional benefit occurred when the nonselective nitric oxide synthase inhibitor was administered.
The area of shock and its pathophysiology and management continues to elicit great research and clinical interest. Poomthavorn and colleagues [87] evaluated adrenal function in critically ill children using total cortisol (TC), free cortisol (cFC), and the free cortisol index (FCI). Basal and peak TC, FCI, and cFC of critically ill children were significantly higher than those of controls, however elevations in basal and peak FCI and cFC were greater than those for TC. FCI and cFC appear to better reflect the dynamic changes of adrenal function of critically ill children than does TC alone. Akker et al. [88] studied glucocorticoid receptor (GR) mRNA levels on day 0, 3, and 7 and during recovery. They determined that there was a transient depression of GR mRNA on day 0 compared with recovery. This may, as the authors hypothesize, reflect a tissue-specific response during sepsis leading to increased cortical resistance of neutrophils. Further work in the area of GC receptor kinetics is likely to improve our understanding of pediatric sepsis and is an attractive field for further research. Khemani et al. [89] described a severity score for use in children with disseminated intravascular coagulation (DIC). They determined that DIC score of ≥5 (severe DIC) in their population of children <18 years with sepsis was associated with a mortality rate of 50% compared with 20% with a lower DIC score, regardless of initial severity of illness (assessed by PIM 2 and PRISM3) or inotrope use. Groselj-Grenc et al. [90] compared the diagnostic accuracy of neutrophil and monocyte CD64 indices for diagnosis of sepsis in critically ill neonates and children with established markers lipopolysaccharide binding protein (LBP), C-reactive protein (CRP), and procalcitonin (PCT). The authors reported the highest diagnostic accuracy for sepsis on the first day of suspected sepsis in their study was achieved by LBP in neonates and by CD64 in children, and 24 h later by CD64 in neonates and children. The importance of this study is to emphasize that gold-standard septic markers may change between age groups, and their usefulness, particularly to support exclusion of sepsis, needs to take account of studies such as this.
Rey et al. [91] reported a nice, simple prospective study of mechanical complications associated with placement of central venous catheters (CVC) in a pediatric intensive care unit. Eight hundred twenty-five catheters were placed in 546 children, having a median age of 22 months. Two hundred ninety-three (35.5%) catheters were sited in the internal jugular, 116 (14.1%) subclavian, and 416 (50.4%) in femoral veins. The most common early mechanical complications were arterial puncture (n = 60; 7.2%), catheter malposition (n = 39; 4.7%), arrhythmias (n = 19; 2.3%), and hematoma (n = 12; 1.4%). Resident failure to perform CVC, high venous access (subclavian or jugular), and number of attempts were independently associated with early mechanical complications (EMC). The value of this study is its sequential and prospective nature, providing as it does a contemporary snapshot, against which others may wish to benchmark, especially as ultrasound guidance for catheter placement becomes more frequent.
Studies by Latour et al. [92] and Vivian et al. [93] focus our attention on the holistic elements of critical care. Latour reported on the use of a multicentre study conducted in seven Dutch PICUs in which parents of critically ill children were asked at follow-up to rate their satisfaction with care received. Studies such as this will allow care to be delivered in ways which are not only clinically safe and effective, but also in ways which best match parental expectations, enhancing collaboration between parents and the PICU clinical team. Vivian et al. assessed the care-giving practices of health-care practitioners in the PICU using qualitative research methodologies. All staff members were active collaborators and were encouraged to describe problems with respect to relationships, trust, and decision-making within care-giving practices. The study qualitatively describes how poor communication amongst staff members in respect of relationships and decision-making impacted on trust and how this can compromise clinical practices in the PICU. The strength of the study was that the participatory design allowing staff members to address the very dynamics that they themselves cited as problematic, rather than the “observational” or “questionnaire” and similar techniques typically deployed in quantitative research in this area.
This section encompasses two main areas: ethics and regulation of clinical research, and end of life in the ICU.
Obtaining informed consent from critically ill patients remains a conundrum, from ethical as well as legal perspectives. Scales et al. [94], from Canada, surveyed 240 patients who were survivors of critical illness. Four scenarios of clinical trials with different possibilities for consent were submitted to participants, who were asked to choose the one they found acceptable or not to them. The main finding that a vast majority (90%) of them favored consent granted by a substitute decision-maker (SDM) is certainly encouraging and should guide lawmakers. However, it is noteworthy that, when the scenario described an emergency situation in which no SDM was present, only 25% of respondents accepted a waiver of consent. However, a majority of them chose or were neutral regarding delayed consent. Burns et al. [95] challenge again the concept of SDM consent for research in emergency situations. They quote several studies which have demonstrated that the requirement for SDM consent may slow the accrual rate or even stop important trials. They argue that too restrictive regulations intended to protect patients may in fact be a “disservice” to this population.
The functioning of Research Ethics Committees (REC) is also a burning issue. Daniel Pehboeck et al. [96] from Innsbruck, Austria, made an online survey of investigators from European Union (EU) member states (n = 193), with the aim of quantifying the level of (dis)satisfaction regarding several administrative constraints. Respondents gave negative ratings to all aspects of the role and actual services provided by RECs. In two papers coming from Austria, prominent members of the Ethics Committee of the Medical University of Vienna gave additional insights into European RECs. Druml et al. [97] provide a comprehensive view of the situation of EU RECs, 5 years after the implementation of directive 2001/20/EC in national legislations. They review the composition of RECs, the number of members, the number of committees per country (from 40 in France to 264 in Italy), and other characteristics. In another article, the same group [98] describe a process they implemented in their REC to deal with the mounting flux of protocols they are dealing with. An expedited review allows a research project to be examined by a subgroup of members of the committee, in order to streamline the process. To be eligible for such “fast-track” review, protocols must entail only “minimal risks” to participants. In an editorial to these two papers, Chassany [99] reminds us that the EU directive and all provisions it sets up cover only the area of drug research, and that the rest of clinical research, on medical devices, surgery, pathophysiology, etc., is totally left out. This is a major cause for the heterogeneity of ethical review within the EU. Additionally, he appeals to the recognition within the directive 2001/20/EC of a simplified track for research with only “minimal risk”. This is a major issue for the revision of the directive, expected for 2011.
Family satisfaction is a recognized marker of quality-of-care assessment. Stricker et al. [100] analyzed 996 questionnaires filled in by family members of ICU patients in German-speaking Switzerland. Globally, care and information/decision-making items had rather high rates. Not very surprisingly, family satisfaction had low rates when the patient-to-nurse ratio was high. Interestingly, higher simplified acute physiologic score (SAPS) scores were correlated with higher satisfaction with care. However, there is some room for improvement: poor grades were given concerning the lack of waiting room, short visiting hours, and information only on demand, giving useful indications where efforts should be directed. On the same line, Fumis [101] investigated the levels of anxiety and depression of families of cancer patients admitted to an ICU in Brazil. They found a high degree of anxiety (71% of families) and of depression (75%). Organizational items were not tested in this study.
Going home to die (GHTD) is supposed to be a universal demand, though it is rarely achieved or even envisaged. However, some ICUs, probably from specific cultures/ethnicities, succeeded in making it possible. Huang et al. [102] report from a single institution in Taiwan how going home to die is achieved as frequently as in 44% (in 2003; a decline over time—25% in 2007—is not commented on by the authors) according to cultural/religious Chinese traditions. It is related to older age, married status, and no do not resuscitate (DNR). From an editorial viewpoint, this paper shows how informative and useful single-center studies can be, at odds with most editorial policies, which praise most multicenter ones. In a joint editorial, Kompanje [103] insists on the feasibility of the process, which necessitates a specialized transportation system, apparently existing in The Netherlands. The GHTD procedure is also mentioned in a paper by Mani et al. [104], which describes the end-of-life decisions in an Indian ICU (also a single ICU report). However, the context here is markedly different: out of the 48 deaths with treatment limitation, 38 (79%) were discharged terminally at home as “left against medical advice,” due to financial reasons. The authors specify that this practice has now disappeared from their ICU. Azoulay et al. [105] have used the SAPS 3 database to identify organizational predictors of decisions to forgo life-sustaining therapies (DFLST). To do so, they analyze data from 14,488 patients admitted to 303 ICUs, out of which 1,239 (8.6%) had a DLFST. They demonstrated that, besides characteristics linked to patients or countries, organizational factors indeed play a significant role. For instance, the decisions to forgo LS therapies appeared more common in hospitals without an emergency department, in smaller ICUs, and in ICUs with a lower nurse-to-patient ratio. DFLSTs were less frequent in ICUs that had at least one full-time intensivist and when they were not available at night or during weekends, which is certainly disturbing at first sight and deserves to be further explored.
Italy remains a country in which decisions at the end of life are still fiercely debated. Striano et al. [106] come back to the Eluana Englaro story, which ignited the peninsula in 2008. They show how withdrawal of fluids and nutrition for patients in permanent vegetative states is still a difficult issue, confused with cultural and religious beliefs.
Brain death and organ donation are also a matter of major interest for intensivists and directly impact on ICU workload. Joffe et al. [107] surveyed medical and nursing students in a medical ethics class, and philosophy students in Canada. They tested their knowledge and basic understanding concerning brain death reality and donation legitimacy. The results are appalling. After presentation of a scenario of a brain death patient, 50% (students who were given brief information) to 65% (those who received detailed information) of them were not convinced that the patient (the donor) was really dead. In the same area, Zamperetti and Bellomo [108], in a thoughtful comment to a white paper of the US Council of Bioethics published in December 2008, draw our attention to the possible evolution of the definition of “brain death” which could impact our practice.
Niskanen et al. designed an observational cohort study (63,304 patients in 23 Finnish ICUs) to create a tool for benchmarking with respect to case-mix-adjusted length of stay (LOS) and to study the association between clinical and economic measures of ICU performance [109]. Linear regression was used to create a model that predicted ICU LOS. There was no association between the mean observed–mean expected ICU length of stay and standardized mortality ratios of the ICUs.
Sacanella et al. recruited prospectively 230 patients ≥65 years living at home, with full autonomy, without cognitive impairment, and admitted to a medical ICU [110]. Over a mean follow-up of 522 days (range 20–1,170 days) the cumulative mortality of the whole group was 55%, being significantly higher in the 120 subjects older than 75 years (62% versus 47%; P = 0.024). On multivariate analysis, only parameters related to quality of life and functional status were independent predictors of cumulated mortality.
In a prospective study, McLaughlin et al. assessed the cost of 64 consecutive admissions to an adult ICU using bottom-up costing methodology and evaluate the usefulness of “severity of illness” scores in estimating ICU cost [111]. The median daily ICU cost was 2,205 euros (1,932–3,073 euros), and the median total ICU cost was 10,916 euros (4,294–24,091 euros). ICU survivors had a lower median daily ICU cost compared with ICU nonsurvivors. Requirements for continuous hemodiafiltration, blood products, and antifungal agents were associated with higher daily and overall ICU costs. Each point increase in SAPS3 was associated with an increase in total ICU cost. A model including hemodiafiltration, blood products, and antifungal agents explained 54% of the variance in total ICU cost.
Oliver et al. identified associations among hemoglobin (Hb) concentrations, blood transfusions, and clinical outcomes in 1,216 patients who underwent valve replacement or bypass surgery [112]. Organ dysfunction, ICU length of stay, and mortality were higher in the two lower quartiles of minimal hemoglobin. Mortality was higher in patients transfused ≥4 packed red cells (PRCs). Patients who underwent bypass surgery when they had Hb ≤8.9 g/dL and those who underwent valve replacement when they had Hb >8.9 g/dL and were transfused ≥4 PRCs had higher mortality.
Roques et al. studied 6-month outcomes in 105 patients with extensive lung cancer [113]. ICU admission was required for acute respiratory failure in 59% of the cases. ICU, hospital, and 6-month mortality rates were 43%, 54%, and 73%, respectively. Cancer progression and the need for mechanical ventilation were independently associated with 6-month mortality. Two-thirds of the ICU survivors were able to receive anticancer treatment.
An optimal target for glucose control in ICU patients remains unclear. Preiser and Coll [114] published a prospective randomized controlled trial comparing the effects on ICU mortality of intensive insulin therapy (IIT) with an intermediate glucose control.
From 1,101 admissions, the outcomes of 542 patients assigned to group 1 [target blood glucose (BG) 7.8–10.0 mmol/L] and 536 patients assigned to group 2 (target BG 4.4–6.1 mmol/L) were analyzed. The groups were well balanced. BG levels averaged in group 1 8.0 mmol/L (IQR 7.1–9.0) (median of all values) and 7.7 mmol/L (IQR 6.7–8.8) (median morning BG) versus 6.5 mmol/L (IQR 6.0–7.2) and 6.1 mmol/L (IQR 5.5–6.8) for group 2 (P < 0.0001 for both comparisons). The percentage of patients treated with insulin averaged 66.2% and 96.3%, respectively. Proportion of time spent in target BG was similar, averaging 39.5% and 45.1% [median (IQR) 34.3% (18.5–50.0%) and 39.3% (26.2–53.6%)] in groups 1 and 2, respectively. The rate of hypoglycemia was higher in group 2 (8.7%) than in group 1 (2.7%, P < 0.0001). This trial was prematurely stopped due to a high rate of unintended protocol violations, and therefore the study was underpowered. The authors concluded that ICU mortality was similar in the two groups (15.3% versus 17.2%), with the lack of clinical benefit of intensive insulin therapy (target 4.4–6.1 mmol/L), associated with increased incidence of hypoglycemia, as compared with a 7.8–10.0 mmol/L target.
Calvino Günther et al. evaluated the incidence of unintended tube, line, and drain removals and identified system factors associated with these events [115]. Of 2,007 admitted patients, 193 (9.6%) experienced 270 events (22/1,000 patient days). Admission for coma and mechanical ventilation were associated with iatrogenic events. SAPS II >45 was protective from iatrogenic events. A continuous quality improvement program allowed a 67.4% drop in the iatrogenic rate.
Pleural effusions are common in ICU patients. Liang et al. [116] retrospectively assessed efficacy and complications of ultrasound-guided pigtail catheter drainage of pleural effusions in ICU. Thoracic empyema and massive transudative effusions were the main reasons for drainage. Drainage of massive transudative effusion yielded the largest amount of fluid, provided the longest duration of drainage, but also had the highest complication rate. Success rate was the highest with traumatic hemothorax and the lowest with empyema. No significant insertion complications were observed. The authors concluded that this technique is well tolerated and effective for draining pleural effusions in ICU.
