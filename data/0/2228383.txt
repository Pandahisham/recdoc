Year in review in Intensive Care Medicine, 2007. I. Experimental studies. Clinical studies: brain injury and neurology, renal failure and endocrinology


The “Experimental studies”, published either as original papers or as brief communications, focused on four main subjects, i. e. the mechanisms of lung injury and therapies, the treatment of burns and trauma, multiple organ failures and the pathophysiology and treatment of sepsis, severe sepsis and septic shock.
Aspiration of gastric contents frequently results in acute lung injury and acute respiratory distress syndrome (ARDS). Because the acid aspiration model of acute lung injury has considerable clinical implications, Fraisse et al. [1] sought to define the relationship between oxygenation and hemodynamic profile during an acute phase of acid aspiration in rabbits. Inhalation of human gastric juice (1 ml/kg) markedly decreased PaO2/FiO2 ratio and increased airway plateau pressure within 1 h. Lung injury was characterized by pulmonary edema, hemorrhage, necrosis, polymorphonuclear leukocyte infiltration, and hyaline membranes, using a PaO2/FIO2 threshold value of 150.
The levels of pH, mean arterial pressure, and cardiac output were significantly lower after acid aspiration. However, neither left nor right ventricular dysfunction occurred during the 4-h experiment, and no animal experienced circulatory failure. This study provided Intensive Care Medicine readers with a PaO2/FiO2 threshold. If the P/F ratio is well above the threshold, it suggests that the lung injury and inflammatory responses may be independent of any compromised hemodynamic changes following acid aspiration.
Several studies examined the mechanisms of ventilator-induced lung injury in a variety of animal models, and provided some novel insights for our readers. Eyal et al. [2] tested the hypothesis that alveolar macrophages can initiate ventilator-induced lung injury. The investigators depleted alveolar macrophages by intratracheal administration of liposomal clodronate 48 h prior to mechanical ventilation in rats. The animals were then ventilated with an injurious strategy at a mean tidal volume of 40 ± 0.7 ml/kg and zero positive end-expiratory pressure (PEEP) for 15 min. The animals were randomized to be sacrificed or to continue being ventilated for an additional 2 h at a tidal volume less than 10 ml/kg. The study demonstrated that oxygenation, lung compliance, and alveolar stability were better preserved during the initial 15-min and the subsequent 2-h ventilation in animals with alveolar macrophages depleted. The lung wet/dry ratio was significantly reduced in the animals with alveolar macrophage depletion compared to the control groups. This study suggested that alveolar macrophages play an important role in initiating ventilator-induced lung injury.
Many critically ill, ventilated patients suffer from unexplained immunosuppression that is associated with increased risk for infections. Lachmann et al. [3] investigated the effects of ventilatory strategies on bacterial translocation in piglets. Lung injury was induced by surfactant depletion, followed by infection with intratracheal injection of group B streptococci. The animals were then ventilated for 5 h with either a conventional ventilatory strategy (tidal volume of 7 ml/kg and PEEP of 4–5 cmH2O), an open lung strategy (6 ml/kg and 14–15 cmH2O), or an open lung with high-PEEP strategy (6 ml/kg and 20–21 cmH2O). The investigators observed that the open lung ventilation resulted in significantly less bacterial translocation than either conventional or open lung with high-PEEP ventilation. This study suggested that an optimal ventilatory strategy (i. e., open lung) achieved by using individually tailored settings can be useful to minimize bacterial translocation during mechanical ventilation.
A recent cohort study reported that non-depolarizing neuromuscular blocking agents are used in 13% of mechanically ventilated patients with a mean duration of 2 days in the ICU. Testelmans et al. [4] studied the effects of 24 h infusion of rocuronium and cisatracurium on diaphragm function in rats during mechanical ventilation using a tidal volume of 6 ml/kg. They observed that the diaphragm tetanic force was decreased by 33% in the rocuronium-treated group compared to a vehicle control group, while the force was well preserved in the cisatracurium-treated animals. The levels of diaphragm muscle RING-finger protein-1 (MURF-1) mRNA and the protease calpain were increased after rocuronium, while unchanged in the cisatracurium-treated groups. Since MURF-1 and calpain are molecules that may contribute to muscle atrophy, these data suggest a role of certain non-depolarizing neuromuscular agents in ventilator-induced lung injury.
Endotracheal suctioning serves to clean the airway in ventilated patients but may cause adverse effects. To assess the effects on lung volume and compliance changes during suctioning, Lindgren et al. [5] monitored lung volume in four ventral-to-dorsal regions by using electric impedance tomography and a nitrogen washout/washin technique in surfactant-depleted, ventilated pigs. At disconnection, before suctioning, the fractional residual capacity (FRC) decreased by 60% of baseline. During open suctioning, FRC decreased further to 20% of baseline. A deterioration of regional compliance was more pronounced in the dorsal parts of the lung. The authors also reported that the restoration of oxygenation was prolonged under pressure-controlled ventilation compared to volume-controlled ventilation during the post-suction period. It appeared that the dorsal regions of a lavaged lung are more susceptible to disconnection and suctioning, resulting in marked decreases in compliance. Under volume-controlled ventilation, the collapsed lung and oxygenation can be better restored from open suctioning than under pressure-controlled ventilation.
Hemodynamic alterations during mechanical ventilation may contribute to poor organ perfusion that leads to multiple system organ dysfunction. Duperret et al. [6] investigated the effect of gradually increased intra-abdominal pressure on the systolic and pulse pressure variations during mechanical ventilation in pigs in the presence and absence of hypovolemia by blood withdrawal. For a given intra-abdominal pressure, the systolic pressure variations were greater in hypovolemic animals than in normovolemic animals. Also, blood flow in the inferior vena cava diminished as the intra-abdominal pressure increased. This study suggested that systemic hemodynamics can be greatly altered by intra-abdominal pressure that leads to changes in pleural pressure, and this effect is more pronounced during hypovolemia.
Enhanced procoagulant and depressed fibrinolytic activities may be implicated in the pathogenesis of ARDS. Plasma protein C levels decreased in ARDS patients compared to normal volunteers. Thrombomodulin levels in pulmonary edema fluid of ARDS patients were higher than in normal plasma and plasma from ARDS patients. Richard et al. [7] tested the hypothesis that if occluded pulmonary vessels reopen with activated protein C (APC), the ventilation-perfusion would be improved in injured lung. Pigs were treated with intravenous infusion of APC starting 30 min prior to oleic acid injection. The animals were monitored for an additional 180 min under mechanical ventilation. PaO2/FIO2 was significantly lower in the APC-treated group before and after induction of lung injury than in a placebo group. Lung perfusion tended to redistribute towards dorsal regions with APC. Total aerated lung volume was identical in the placebo and the APC-treated animals before and after the injection of oleic acid. The plasma concentrations of IL-6 and IL-8 were higher in the APC group 2 h after oleic acid injection than in the placebo group. In contrast to sepsis models, pretreatment with APC resulted in worsening oxygenation. The author speculated that the administration of APC might have led to ventilation-perfusion mismatch, with more perfusion to dependent non-aerated areas in their model. It is noteworthy that APC exerted beneficial effects only in those critically ill patients. The present animal model of oleic acid injection might not have reproduced severe situations seen clinically.
The above studies addressed mechanisms and discussed several practical issues in the context of ventilator-induced lung injury. Theoretically, lung-protective strategies may be the most important primary approach to minimize ventilator-induced lung injury. This concept is supported by the results of the ARDSnet trial showing a significant reduction in the absolute mortality rate when lower tidal volumes were used compared to higher tidal volumes. Muellenbach et al. [8] proposed that high-frequency oscillatory ventilation (HFOV) may be an optimal lung-protective strategy to minimize ventilator-induced lung injury. They investigated the effects of 24-h mechanical ventilation by using either lung-protective pressure-controlled ventilation (tidal volume of 6 ml/kg) or HFOV (6 Hz) in a pig model of ARDS induced by repeated lung lavage. This study showed in a large animal model of ARDS that the lung inflammation score and the expression of IL-1β mRNA in lung tissue were significantly decreased in the HFOV-treated animals compared to the lung-protective pressure-controlled ventilated group.
Acute kidney injury secondary to sepsis and septic shock affects approximately 6% of critically ill patients. It has been proposed that high resistance in the renal vasculature resulted in renal hypoperfusion in a variety of animal models in which the study designs were heterogeneous. To understand the nature of renal blood flow and its function during recovery from experimental septic acute kidney injury, Langenberg et al. [9] recorded systemic and renal hemodynamics during septic acute kidney injury and subsequent recovery in sheep. Sepsis was induced by the administration of a bolus of live Escherichia coli followed by a continuous infusion for 48 h. Normal saline was administered to prevent marked hypovolemia. A bolus of gentamicin was injected and E. coli infusion was stopped. The animals were then monitored for 48 h during recovery phase with fluid administration continued. Infusion of E. coli induced a hyperdynamic circulatory state with increased cardiac output and decreased blood pressure. Acute kidney injury was evidenced by decreased creatinine clearance. Renal vasodilatation occurred that was accompanied by an increase in renal blood flow. During recovery, renal blood flow returned to control levels associated with relative renal vasoconstriction. Indices of tubular function decreased during sepsis and returned to control values after 18 h of recovery. Although the mechanisms responsible for the development of recovery from acute kidney injury are yet to be elucidated, the results of this study nevertheless challenge the conventionally held paradigm of renal ischemia as the underlying cause for the development of septic acute kidney injury.
Ischemia and reperfusion is associated with excessive inflammatory responses, and the mechanisms are very complex. Glycogen synthase kinase-3β (GSK-3β) is a key regulatory enzyme in glucose metabolism that, when activated, phosphorylates/inactivates target enzymes of the insulin signaling pathway. It has been recently reported that GSK-3β is involved in the regulation of many cell functions, but its role in the regulation of the inflammatory response remains unknown. Cuzzocrea et al. [10] investigated the effects of TDZD-8, a potent and selective GSK-3β inhibitor, on intestinal injury following ischemia/reperfusion induced by splanchnic artery occlusion in rats. TDZD-8 was administered 5 min prior to the 6-h reperfusion phase. The investigators observed that the TDZD-8-treated animals had higher blood pressure and a greater survival rate than a group receiving vehicle control. These beneficial effects were associated with a decrease in neutrophil infiltration in the reperfused intestine, attenuation of the production of inflammatory cytokines and the degranulation of IκB-α, reduction of oxidative stress, and prevention of cell apoptosis. This study opens a new avenue to explore the role of GSK-3β as a therapeutic target in ischemia and reperfusion.
Recent findings suggest that abdominal compartment syndrome, as reflected by intra-abdominal hypertension (IAH), is associated with an increased rate of the occurrence of multiple organ dysfunction syndrome. Prevention of abdominal compartment syndrome has been shown to decrease the incidence of multiple organ dysfunction. The inflammatory cytokine responses are crucial to trigger multiple organ dysfunction in abdominal compartment syndrome, but studies suggested that decompression may be too late to reverse the inflammatory responses. Thus early detection and prevention of abdominal compartment syndrome is of pivotal interest in successful treatment. Meier et al. [11] evaluated the potential role of microdialysis to detect intra-abdominal organ injury during IAH by measuring energy metabolism in rats. IAH was induced for 3 h and followed by decompression and reperfusion for another 3 h. The prolonged IAH induced persistent abdominal organ injury. Microdialysis analysis demonstrated a significant increase in lactate/pyruvate and glycerol in kidney, intestine, and liver, indicating ischemia, energy failure, and cell membrane damage. This was accompanied by a decrease in glucose level in all organs studied. The deterioration of energy metabolism did not completely recover upon decompression of IAH. This study suggested that continuous microdialysis in the rectus abdominis muscle may provide a useful tool for early detection of IAH-induced metabolic derangements.
The loss of blood volume would be further increased with an elevated hydrostatic capillary pressure, especially when vascular permeability is increased. If the hypothesis is true, this concept may lead to a potential therapeutic strategy to minimize blood volume substitution in critically ill patients by avoiding high blood pressure. To evaluate interactions between the increased vascular permeability and increased hydrostatic pressure on blood volume loss, Dubniks et al. [12] induced vascular permeability as a result of an anaphylactic reaction by injection of dextran 70 in rats. Plasma volume was measured before and after 5% albumin infusion for volume expansion. Blood pressure was elevated by administration of noradrenalin or decreased by metoprolol/clonidine started after the albumin infusion and continued throughout the experiment. The investigators observed that the plasma volume increased after the albumin infusion in the control group, but to a significantly lesser extent—by 24-fold—in the noradrenalin-treated group in the experimental conditions with increased vascular permeability. On the contrary, the plasma volume increased in the noradrenalin-treated group in the normal permeability state, and this increase was similar in extent to that seen in the metoprolol/clonidine-treated animals under increased permeability conditions. The study provided evidence that an increase in arterial pressure increases the loss of plasma volume during a state of increased permeability, perhaps due to increased hydrostatic capillary pressure.
Head trauma and burn injury are characterized by hypermetabolism and hypercatabolism mediated by altered regulatory mechanisms of hormones, imbalanced pro- and anti-inflammatory cytokine production, and immune depression. It has been shown that decreased production of interleukin-2 (IL-2) and expression of the surface IL-2 receptor alpha chain (CD25) is associated with immune depression in head injury. Arginine is known as a nitric oxide donor and a precursor of polyamines for IL-2 production. Hamani et al. [13] tested the hypothesis that standard enteral nutrition may not be sufficient to meet specific nutrient demand in head trauma. The investigators examined the role of arginine in the immunomodulatory effects in a rat model of head injury. They showed that the standard diet supplemented with arginine reduced thymic atrophy, maintained thymus weight, increased the expression of CD25 and the production of IL-2, and blunted the enterobacterial translocation and dissemination induced by head injury. Although the benefit of immune-enhancing diets in the management of trauma injury remains controversial, this study suggested that arginine supplement appears to be safe, and may help modulate immune response in head injury.
Ornithine α-ketoglutarate (OKG) has been shown to restore glutamine pools in hypercatabolic patients and to improve wound healing in trauma patients, but the mechanisms remain to be elucidated. Since OKG consists of two molecules of ornithine (Orn) and one molecule of α-ketoglutarate (αKG), Cynober et al. [14] tested the hypothesis that Orn and αKG interact metabolically to produce glutamine in muscles in a rat model of 20% surface burn injury. The animals were fed with enteral nutrition supplemented with either OKG, Orn, or αKG. Glycine was used as an isonitrogenous control. Higher plasma content of glutamine and muscle contents of glutamate and glutamine were observed in the animals treated with the OKG-enriched diet than in those that received diets supplemented with αKG or Orn. This study demonstrated that OKG was more efficient than Orn or αKG alone in restoring glutamine pools in plasma and muscle, suggesting that the metabolic interaction between Orn and αKG is required to produce glutamine in muscles.
Animal models of sepsis and acute lung injury were used in several studies published in Intensive Care Medicine during 2007. Although not perfect, these models were long ago shown to be valuable to test pathogenic hypotheses and potential therapies in sepsis and acute lung injury.
Left ventricular contractile dysfunction is observed in a significant proportion of patients with septic shock. This sepsis-related cardiac failure is usually transitory and reversible. Given the high peripheral oxygen demand and the vasodilatory shock, oxygen delivery may be insufficient in the early phase of septic shock in those patients. A pharmacologic increase of cardiac output may therefore be required in such patients. Dobutamine has been the inotrope of choice for several years in this condition. It is also associated with unwanted ß-agonistic effects, which may limit its use. Dubin et al. tested dobutamine and the new inotrope levosimendan head-to-head in an endotoxemic shock in sheep model [15]. Both drugs increased systemic oxygen delivery by increasing cardiac output. However, only levosimendan induced an increase of intestinal blood flow and prevented the decrease in mucosal acidosis that was observed with dobutamine. This study, along with others, serves as a proof-of-concept that levosimendan may become a useful drug to treat patients with septic-associated left ventricular dysfunction. Energy utilization by the heart is also a critical aspect of cardiac function during sepsis. The heart is one of the few organs using lactate as energetic substrate. Levy and collaborators examined whether lactate deprivation induced a cardiac and hemodynamic dysfunction in a rat endotoxic shock model [16]. The combined pharmacological blockade of the ß2 receptor and pyruvate dehydrogenase markedly decreased muscle production of lactate, heart lactate concentration, and tissue ATP content. This was associated with a worsening in cardiovascular performance and a poorer outcome. These effects could be reversed by the infusion of lactate. This study highlights the role of lactate as an important fuel for the heart and suggests that lactate deprivation might be detrimental during septic shock.
Endotoxemia induces a marked increase of both pro-inflammatory and anti-inflammatory cytokines, which are though to be detrimental in this setting. Many efforts have been devoted to decrease cytokine production or increase cytokine elimination in sepsis models. It remains unclear whether putative beneficial effects of activated protein C (aPC) are related to its anti-thrombotic and pro-fibrinolytic properties or to possible anti-inflammatory effects. In an porcine model of endotoxemia, Nielsen et al. confirmed a pro-fibrinolytic effect of aPC by decreasing plasma levels of plasminogen activator inhibitor 1 [17]. However, the effects on circulating cytokines were modest; aPC infusion did not influence peak levels of IL-6, IL-8, TNF-a, and IL-10, the peak occurring simply later for the two latter. This is in accordance with the PROWESS study, which did not show marked plasma cytokine level differences between patients receiving aPC and controls. Endotoxemia also induces lung inflammation, a phenomenon largely mediated by locally produced IL-1ß. The secretion of mature and bioactive IL-1ß and IL-18 depends on the activity of the enzyme caspase-1, a limiting step in the biosynthesis of these pro-inflammatory cytokines. In a series of elegant experiments, Boost and collaborators showed that a caspase-1 inhibitor administered by inhalation in endotoxemic rats decreased IL-1 and IL-18 levels in bronchoalveolar lavage fluid [18]. Importantly, downstream inflammatory products such as COX-2 and nitric oxide were also decreased in animals treated with the caspase inhibitor. In addition to the identification of a potential novel therapeutic target (caspase-1), this study highlighted inhalation as a promising route for the administration of drugs in critically ill patients.
Hemoperfusion columns are theoretically interesting to remove noxious bacterial or host substances during septic shock. Taniguchi et al. perfused blood from lipopolysaccharide (LPS)-treated rats onto a cytokine absorbent column, which resulted in a significant reduction in plasma cytokine concentrations, and decreased mortality, compared with controls [19]. Survival rates were dependent on the size of the column. Oxidative stress-induced programmed cell death has been proposed as a mechanism participating in end-organ dysfunction during sepsis. Ozdemir et al. showed that infant rats injected with endotoxin had increased levels of lipid peroxidation markers and intestinal cell apoptosis [20]. These effects were dampened by the treatment with melatonin. This apparently harmless molecule should be further tested for its potential role as an anti-oxidant and for putative cytoprotective effects in the context of sepsis. In a rat model of septic shock induced by the staphylococcal alpha-toxin, Temmesfeld-Wollbrück et al. showed that post-treatment with the vasoregulatory polypeptide adrenomedullin decreased mortality [21]. The main effect of adrenomedullin was to prevent the massive increased vascular permeability observed in this model, as demonstrated by a smaller diffusion of labeled albumin in remote organs of treated animals. Mortality was also markedly reduced in animals receiving adrenomedullin. Staphylococcal alpha-toxin, being a pore-forming toxin, is likely to induce significantly more vascular damage than other toxins such as LPS. Therefore, the effects of adrenomedullin need to be further investigated in endotoxemic or infectious models.
Cecal ligation and puncture (CLP) is a useful model, mimicking peritonitis. CLP induces lethality rates depending on the size of the hole(s) performed in the cecum of mouse or rat. The advantage of this model over endotoxemia is that live bacteria represent the infectious challenge. Nitric oxide (NO) is produced in excess during septic shock, due to the increased expression of inducible NO synthase (iNOS). In a resuscitated CLP model, Albuszies et al. showed that glucose production by the liver was higher in mice treated with a specific iNOS inhibitor and in mice deficient for the iNOS gene than in littermates [22]. Increased hepatic activity of phosphoenolpyruvate carboxykinase—a key enzyme of gluconeogenesis—paralleled the increased glucose production. These results suggest that iNOS-induced excess production of NO during septic shock may down-regulate the glucose production by the liver. High glucose production by the liver was maintained when iNOS was blocked or genetically absent during CLP. It has also been suggested that NO was an important molecule participating in the innate control of bacterial infections. Cui et al. showed that the pharmacological inhibition and the gene deletion of the neuronal NOS isoform increased plasma pro-inflammatory cytokine concentrations and mortality in a CLP mouse model [23]. These results suggest that this NOS isoform is important for bacterial clearance and cytokine production during severe bacterial infections. In a P. aeruginosa rat model, inhaled NO (iNO) increased pulmonary endothelial but not epithelial permeability [24]. This effect could not be related with modifications in levels of alveolar inflammation. Further studies are needed to unravel mechanisms of iNO-induced increase in endothelial permeability in this model. In an original study, Tuon et al. submitted rats to CLP, followed by fluid resuscitation and antibiotic treatment [25]. They reported that rats exhibited depressive behavior 10 days after sepsis, as assessed by a forced swimming test. Interestingly, these symptoms could be reversed by the treatment with the anti-depressive drug imipramine. Dehydroepiandrosterone (DHEA) has been shown to improve the outcome in some models of systemic inflammation, although the mechanisms remain unclear. Oberbeck et al. showed that parenteral administration of DHEA (modestly) improved survival in a CLP mouse model. Increased heat shock protein-70 levels, decreased splenocyte apoptosis, and TNF release observed in treated animals may participate in the protective mechanism of DHEA [26].
Although the nature of fluid to be used for volume resuscitation remains a question of debate, starches have gained popularity in some European countries during the past decade. However, both the plasma volume expansion capacity and the secondary effects seem to be governed by their molecular weight and degree of hydroxyethyl substitution. In a pig model of hemorrhagic shock, Eisenbach et al. demonstrated that three different hydroxyethyl starch solutions with molecular weight between 100 and 200 kDa restored macro- and microcirculation similarly [27]. Urine production was, however, higher in a starch preparation of lower molecular weight, whereas plasma clearance was less in starches with a molecular weight of 200 kDa. As a note of caution, all starch solutions had significantly accumulated in various organs by 6 h after infusion. Another controversy is the use of normotonic vs. hypertonic saline solution for volume resuscitation, particularly when effects on lung injury are measured. In a similar shock model in pigs, Roch et al. showed that acute hemorrhage induced a significant lung injury assessed histologically with increased alveolar inflammation and edema [28]. However, when a strict goal-directed resuscitation protocol was applied, lung injury parameters did not differ whether animals were resuscitated with normal saline or with small volumes of hypertonic/hyperoncotic saline.
It has been suggested by various clinical investigators that treatment with antibiotics before bronchoalveolar lavage sampling will jeopardize results that can be obtained with this diagnostic procedure. Brandão da Silva et al. modeled this situation in pneumonia models in rats [29]. Not surprisingly, it was found that pretreatment with antibiotics markedly decreased the diagnostic yield of BAL cultures in animals with S. pneumoniae or P. aeruginosa pneumonia. The direct examination of BAL smears, however, showed that a significant proportion of cases of pneumonia could still be diagnosed based on the detection of bacteria inside neutrophils (intracellular microorganisms) despite pretreatment with antibiotics. The importance of sampling patients with suspected pneumonia before initiation of a novel antibiotic therapy was highlighted in an accompanying editorial by Timsit [30].
In a well-described ovine model of lung injury (smoke inhalation plus bacterial pneumonia), Maybauer and collaborators reported that ceftazidime showed numerous beneficial effects besides its antibacterial properties [31]. Ceftazidime treatment was associated with hemodynamic stabilization, better oxygenation, and less bronchial obstruction. Interestingly, ceftazidime blunted the increase in 3-nitrotyrosine observed in lungs from control animals.
Several articles dealt with treatment, pathophysiology and prognosis of traumatic brain injury (TBI).
Following TBI, impaired autoregulation contributes to the increased sensitivity of the brain to secondary ischemic insults, especially those caused by hypotension. The concept of an individualized treatment is emerging, targeting the optimal cerebral perfusion pressure (CPP) strategy on the basis of an impaired or intact autoregulation. Chieregato et al. [32] exploited an imaging technique for evaluating the relationship between CPP and cerebral blood flow (CBF). Using xenon-CT in 162 patients with severe TBI, they evaluated the association between global CBF and different CPP, within the 50–90 mmHg range. No correlation was evident between CPP and global cerebral blood flow in this selected cohort, probably because most of the patients were studied in a normal CPP range, and pressure autoregulation (not tested by the authors) was preserved.
Unfortunately testing autoregulation clinically is a complex task. Dynamic methods of measuring cerebral autoregulation have become an accepted alternative to static evaluation, but a reference reliable method is not available in clinical practice. Christ et al. [33] and co-workers tried to improve the cross-correlation method for non-invasive, continuous monitoring of cerebral autoregulation, in comparison to the cuff deflation test. Inter-method agreement in diagnosing an intact or impaired cerebral autoregulation was obtained in 73.5% of examinations. Cross-correlation analysis might serve as a simple, non-invasive, and continuous measure of cerebral autoregulation. Nevertheless, as suggested by the authors, short-term autoregulation tests and monitoring techniques based on slow spontaneous oscillations should not be used interchangeably. Therefore the best method for continuous assessment of autoregulation state remains undefined and requires additional investigations.
Two studies of a very early post-TBI phase explored the possibility of non-invasive monitoring of intracranial pressure (ICP) and of CPP target treatment, before its invasive monitoring. Geeraerts et al. [34], in order to evaluate a non-invasive method of ICP determination, assessed at ICU admission the relationship between optic nerve sheath diameter (ONSD) and ICP and whether greater ONSD at patient admission was associated with higher ICP in the first 48 h after TBI. A significant relationship between the greatest ONSD and admission ICP was documented and it was a suitable predictor of high ICP. In the early posttraumatic period, ocular ultrasound scans may be attractive for detecting high ICP even if additional demonstrations are required.
Ract et al. [35] evaluated the usefulness of early transcranial Doppler ultrasound (TCD) goal-directed therapy after severe TBI, before the availability of invasive cerebral monitoring. When admission TCD was abnormal, attending physicians increased CPP by infusing mannitol and/or norepinephrine, with normalization of TCD recordings in 9/11 patients. After invasive monitoring, the abnormal TCD group demonstrated higher ICP. The use of TCD at hospital admission could allow the identification of severely brain-injured patients with potential brain hypoperfusion. In such high-risk patients, early TCD goal-directed therapy appears attractive in restoring normal cerebral perfusion and might potentially help in reducing the extent of secondary brain injury.
After ICU admission, standard and advanced monitoring are utilized in order to reduce the incidence of secondary insults and to identify the heterogeneity of this pathology. Longhi et al. [36] measured in 32 TBI patients the brain tissue oxygen tension (PtiO2) in the hypodense area and/or near the core of the contusion and in normal-appearing brain parenchyma on computerized tomography. PtiO2 was lower in pericontusional tissue than in normal-appearing tissue. In pericontusional tissue, a progressive increase of PtiO2 from pathologic to normal values was observed over time, suggestive of a microcirculatory improvement. The use of advanced monitoring system helps the clinician to reach a better understanding of the evolving TBI pathophysiology.
In this complex pathology, early prognosis is sometimes a difficult task. Ballesteros et al. [37] investigated whether serum drained from the jugular vein of patients with traumatic or hemorrhagic brain injury induced apoptosis of neuronal cells in vitro and whether the apoptotic rate correlated with patients' outcome at 6 months. Regional serum drained from the jugular vein induced higher early apoptosis than systemic serum, and only early apoptotic rate was an independent factor associated with mortality at 6 months. This in vitro technique, combined with clinical and radiological measurements, might improve the value of prognostic models to predict acute brain injury patients' outcome. Korfias et al. [38] examined the relationship between serum S-100B concentrations and injury severity, clinical course, survival, and treatment efficacy after severe TBI. Serum S-100B protein reflects injury severity and improves prediction of outcome after severe TBI. S-100B might also have a role in assessing the efficacy of treatment after severe TBI, demonstrating a reduction after surgery.
Updated information on aneurysmal subarachnoid hemorrhage (SAH) patients is not widely available. Citerio et al. [39] collected information on clinical practice and current management strategies on 350 sequential cases in 22 Italian neurosurgical hospitals. Despite the increasing trend towards interventional neuroradiologic means of securing aneurysms, aneurysms were mainly clipped and an endovascular approach was utilized in one third of cases, with wide variation among centers. SAH was confirmed to be a multiorgan disease, with frequent extracranial and intracranial complications. Nevertheless, only high ICP and deterioration in neurological status were independent factors related to unfavorable outcome.
Confirming the multiorgan nature of this pathology, Terao et al. [40] determined the prevalence and the prognostic significance of microalbuminuria in SAH. The prevalence rates of microalbuminuria were higher in SAH than in the control, and the highest urinary microalbumin/creatinine ratio and the lowest GCS score during the first 8 days were the significant predictors of unfavorable neurological outcome.
The number of patients under oral anticoagulant (OAC) therapy, and consequently the number of subject with related intracerebral hemorrhagic (ICH) complications, is increasing.
Appelboam et al. [41] examined current British practice regarding the emergency medical management of ICH patients whilst receiving OAC and to compare this with established guidelines. Prothrombin complex concentrate (PCC), the gold-standard therapy to normalize hemostasis during OAC, remains underused. There is considerable variation in practice amongst clinicians and, in most cases, practice is not in keeping with guidelines.
Vigué studied whether ultra-rapid reversal of OAC could reduce the time to biological and surgical hemostasis, and might improve outcome. All patients, including over-anticoagulated individuals, had complete reversal of anticoagulation immediately after the bolus of PCC. No hemorrhagic or thrombotic adverse effect was observed. A bolus infusion of PCC completely reverses anticoagulation within 3 min. Neurosurgery can be performed immediately in OAC-related intracranial hemorrhage. This study shows that OAC-treated patients can be managed as rapidly as non-anticoagulated patients.
Spinal cord injury after aortic surgery still represents a matter of concern. Maier et al. [42] investigated the effect of the PARP-1 inhibitor INO1001 on aortic occlusion-related porcine spinal cord injury. The selective PARP-1 inhibitor INO1001 markedly reduced aortic occlusion-induced spinal cord injury. INO1001 might improve spinal cord recovery after thoracic aortic cross-clamping and, after these encouraging results, further data are required.
Two papers tested two diagnostic procedures during brain death, i. e. CT angiography (CT-a) and bispectral index (BIS). Quesnel et al. [43] evaluated the accuracy of cerebral CT-a for the diagnosis of brain death. In clinically brain-dead patients, CT-a documented opacification of the intracerebral vessels in a significant percentage of the cases. Therefore CT-a cannot be recommended as a means of brain death diagnosis.
Wennervirta et al. [44] studied the usefulness of entropy and BIS in brain-dead subjects. Both entropy and BIS showed non-zero values due to artifacts after brain death diagnosis. BIS was more liable to artifacts than entropy. Neither of these indices are diagnostic tools, and care should be taken when interpreting EEG-derived indices in the evaluation of brain death.
Sharshar et al. [45] tried to advance in our understanding of sepsis-induced brain dysfunction using in vivo magnetic resonance imaging.
This preliminary study showed that sepsis-induced brain lesions (such as multiple ischemic strokes, white and gray matter lesions) can be documented by magnetic resonance imaging. These lesions predominated in the white matter, worsened with increasing duration of shock, suggesting increased blood–brain barrier permeability, and were associated with poor outcome.
ICU delirium is a common, undiagnosed and adverse event in critically ill patients. Ouimet et al. [46] investigated whether subsyndromal delirium affects outcome. Patients with no delirium were more likely to be discharged home and less likely to need convalescence or long-term care than those with subsyndromal delirium or clinical delirium. A graded diagnostic scale (Intensive Care Delirium Screening Checklist) permitted detection of subsyndromal delirium which occurs in many ICU patients, and which is associated with adverse outcome.
Pandharipande et al. [47] identified the prevalence of the motoric subtypes of delirium in surgical and trauma ICU patients. Prevalence of delirium was 70% for the entire study population. Hypoactive delirium was significantly more prevalent than either mixed or hyperactive delirium. In the absence of active monitoring, however, this subtype of delirium goes undiagnosed and the prevalence of delirium in surgical and trauma ICU patients remains greatly underestimated.
Among the controversies in acute renal dysfunction in the critically ill patient are the mechanisms of renal shutdown and reversibility, the early diagnosis and staging of severity, and issues concerning renal replacement therapy.
Renal arterial resistance in septic shock and the effect of increasing mean arterial pressure with norepinephrine thereon was studied with the help of Doppler ultrasonography by Deruddre et al. [48] in 11 patients with septic shock who required fluid resuscitation and norepinephrine to maintain mean arterial pressure at or above 65 mmHg. Norepinephrine was dosed for three successive periods of 2 h to achieve a mean arterial pressure of 65, 75, and 85 mmHg, respectively. At the end of each period, hemodynamic and renal function variables were measured, and Doppler ultrasonography was performed on interlobar arteries to assess the renal resistive index. With increasing mean arterial pressure, urinary output increased from 76 ± 64 to 93 ± 68 ml/h and the resistive index decreased from 0.75 ± 0.07 to 0.71 ± 0.06. No difference was found between 75 mmHg and 85 mmHg. Doppler ultrasonography may thus help to determine in each patient the optimal mean arterial pressure for renal blood flow and may be a relevant end-point for treatment of septic shock. This study suggests that decreased renal blood flow during septic shock may be a contributing factor to renal dysfunction, which may contrast with current animal data that argue against hypoperfusion as a pivotal event in acute renal failure (ARF) during endotoxemia.
Herrera-Gutiérrez et al. [49] wondered whether early diagnosis of acute renal failure could be based on 2-h as opposed to 24-h creatinine clearance (CrCl) in the ICU. They studied 359 patients and compared the measures, also with the estimated clearance from the Cockcroft–Gault (Ck-G) equation. The mean Ck-G value was 87.4 ± 3.0, with 2-h CrCl of 109.2 ± 4.5 ml/min/1.73 m2 and 24-h CrCl of 100.9 ± 4.2 ml/min/1.73 m2 (r2 = 0.88 for CrCl-2h and r2 = 0.84 for Ck-G). The differences from 24-h CrCl were 21.8 ± 3.3 for the Ck-G and 8.3 ± 2.6 for 2-h CrCl. Patients with CrCl < 100 ml/min only showed variability in hyperglycemia during the 24-h period. Hence, 2-h CrCl is an adequate substitute for 24-h CrCl, even in patients who are unstable or who have irregular diuresis where a 24-h collection is unpractical. The Ck-G equation seems less useful. These ideas can be utilized in attempts to improve prognosis of patients by earlier diagnosis of acute renal failure and subsequent measures to prevent further deterioration.
Jenq et al. [50] studied whether the new RIFLE (risk, injury, failure, loss and end stage) classification can predict short-term prognosis in critically ill cirrhotic patients. This study analyzed the outcomes of critically ill cirrhotic patients and identified the association between prognosis and the RIFLE classification, in comparison with other five scoring systems, in Taiwan. Thirty-two demographic, clinical and laboratory variables were analyzed as predictors of survival in 134 patients. Overall hospital mortality was 66%. There was a progressive increase in mortality based on RIFLE classification severity. Multiple logistic regression analysis indicated that RIFLE classification and Sequential Organ Failure Assessment (SOFA) score on the first day of ICU admission were independent risk factors for hospital mortality. By using the areas under the receiver operating characteristic curve (AUROC), the RIFLE category and SOFA score both indicated good discrimination (AUROC 0.84 ± 0.04 and 0.92 ± 0.02 respectively). Cumulative survival rates at 6-month follow-up differed for non-ARF vs. R, I, and F. This study again shows the prognostic value of staging ARF by the RIFLE criteria.
Maccariello et al. [51] also studied the RIFLE classification in patients with ARF in need of renal replacement therapy. They included 214 patients, and continuous renal replacement therapy was used in 179 (84%); patients were classified as risk (25%), injury (27%), or failure (48%). Overall mortality was 76% but there were no differences according to RIFLE classification (Risk 72%, Injury 79%, Failure 76%). Various variables were selected in multivariate analysis, including start of renal replacement therapy after the first day of ICU, but the RIFLE stages did not contribute. However, a subgroup analysis of patients on mechanical ventilation and vasopressors found the F of RIFLE to be associated with increased mortality. Hence, this study does not confirm the independent prognostic significance of the RIFLE criteria and argues in favor of further refinement.
ARF in critically ill patients still carries high mortality, and a high proportion of these patients require renal replacement therapy (RRT) during their illness. To evaluate the efficiency of RRT, usually urea kinetic modeling is used, based on a sample of dialysate. The accuracy of this estimation in critically ill patients remains, however, questionable, since it assumes stable urea production and constant distribution of urea, which is not the case in critically ill patients. Ionic dialysance, which is a parameter calculated automatically from the dialysate conductivity, has been correlated to the effective urea clearance during hemodialysis in chronic renal failure patients. Ridel et al. [52] evaluated this method in critically ill ARF patients, using 31 sessions of intermittent hemodialysis (IHD) in 31 patients. Comparing the effectiveness of the RRT, they found a strong correlation between the Kt measured with dialysate sampling and with the ionic dialysance method (r = 0.96, p > 0.01). Despite some limitations of their study (small delivered RRT dose, lack of measurement of the removal of intermediate-molecular weight molecules), they conclude that this methodology might be well suited to monitor and adapt RRT in critically ill ARF patients.
Continuous RRT (CRRT) is often the method of choice in critically ill patients, when intermittent methods seem not to be appropriate. CRRT, however, is not a simple single method—it rather consists of a continuum of different treatment modalities which can be distinguished through a variety of factors. Uchino et al. [53] used data available from the B.E.S.T. study to investigate several aspects of CRRT. They included 1006 subjects treated with CRRT for ARF. Interestingly, they reported that approximately a third of the CRRT treatments (33.1%) were carried out without anticoagulation. Among those who received anticoagulation, unfractionated heparin (UFH) was the most common choice, followed by sodium citrate. Only 11.7% received a treatment dose above 35 ml/kg/h, the average being 20.4 ml/kg/h. Hypotension was the most frequently recorded complication during CRRT (observed in 18.8% of the patients), followed by arrhythmias (4.3%) and bleeding episodes (3.3%). With respect to outcome, approximately one third of the patients died on CRRT. Most of the survivors recovered renal function until their discharge from the hospital. The authors concluded that current RRT practice does not follow established guidelines for optimal therapy and thus might be responsible for significant morbidity.
CRRT is associated with less chronic renal failure than intermittent hemodialysis after ARF, according to a large, nationwide retrospective study in Sweden (Bell et al. [54]). The objective of the authors was to determine the impact of type of RRT on renal recovery in patients with acute renal failure. A total of 2,642 patients from 32 ICUs were included. Patients with end-stage renal disease and patients lacking a diagnosis in the inpatient register were excluded. Thus, 2,202 patients were studied and follow-up was complete. Renal recovery and mortality were studied. There were no differences between patients on IHD and those on CRRT techniques regarding baseline characteristics. Of the 1,102 patients surviving 90 days after inclusion, 86% were treated by continuous techniques and 14% by IHD. Seventy-eight patients (8%) never recovered renal function in the CRRT group. The proportion was higher among patients treated by intermittent techniques, where 26 subjects or 16% developed need for chronic dialysis. Mortality did not differ. Nevertheless, these data on a large number of patients support the use of continuous techniques.
Baldwin et al. [55] performed a randomized controlled comparison of continuous venovenous hemofiltration and extended daily dialysis with filtration, with respect to effect on small solutes and acid–base balance. Recently, extended intermittent dialytic techniques have been proposed for the treatment of acute renal failure. Sixteen critically ill patients were subjected in a randomized controlled trial to 3 days of treatment with either continuous venous hemofiltration (n = 8) or extended daily dialysis with filtration (n = 8). There was no difference between the two therapies for urea or creatinine levels. One patient in the continuous group developed hypophosphatemia (0.54 mmol/l) at 72 h. After 3 days of treatment, there was a mild but persistent metabolic acidosis in the extended daily dialysis with filtration group. Hence, in spite of similar control of urea, creatinine and electrolytes, acidosis was better controlled by continuous venovenous hemofiltration. This study adds to the ongoing debate on the superiority of continuous over intermittent techniques.
Another controversial issue relates to the mode of anticoagulation of the filter. Joannidis et al. [56] compared enoxaparin with unfractionated heparin, in a randomized controlled crossover study. Continuous venovenous hemofiltration was performed (predilution; 2500 ml/h ultrafiltration and 180 ml/min blood flow rate). Heparin-treated patients received an initial bolus of 30 U/kg and a maintenance infusion at 7 units/kg/h, dosed to achieve an aPTT of 40–45 s. Enoxaparin-treated patients received an initial pre-filter bolus of 0.15 mg/kg and a maintenance infusion starting at 0.05 mg/kg/h, adjusted to maintain anti-Xa activity at 0.25–0.30 U/ml. Each patient received both regimens in a crossover design. Maximum treatment duration for each set was 72 h, and 37 patients completed both study arms. Mean filter life span was 22 ± 17 h for heparin and 31 ± 25 h for enoxaparin. One major bleeding episode occurred during heparin and one during enoxaparin treatment. Daily costs averaged €270 and €240 for heparin and enoxaparin, respectively. Enoxaparin can thus be safely and cost-effectively used for anticoagulation during continuous venovenous Hemofiltration, resulting in longer filter life than with unfractionated heparin. A next step in this debate could be a comparison with citrate-based filtration.
Continuous hemofiltration techniques carry the disadvantage of losses in the ultrafiltrate. Berg et al. [57] evaluated glutamine kinetics during supplementation and CRRT in 12 patients randomized to receive alanyl-l-glutamine i. v. for 20 h before placebo, or placebo before glutamine, on two consecutive days. Plasma and ultrafiltrate glutamine concentrations were measured, and blood flow across the leg was measured to calculate efflux of glutamine. Glutamine supplementation increased plasma concentrations. Losses into the ultrafiltrate were similar during treatment and control days. Net glutamine balance across the leg was also similar on treatment and control days. The loss of glutamine into the ultrafiltrate suggests an augmented need for exogenous glutamine. Although supplementation did not decrease endogenous production, it did not aggravate losses via the ultrafiltrate. Hence, in dosing of glutamine supplementation in patients on continuous venovenous hemofiltration these factors should be taken into account.
Current paradigms in intensive care medicine include the idea, albeit not beyond doubt, that tight glucose control and selected treatment by substitution doses of hydrocortisone for relative adrenal insufficiency (RAI) improve outcome. However, the mechanisms of such benefits and optimal ways to achieve them in practice are still open to discussion.
Weber-Carstens et al. [58] studied the effect of low-dose hydrocortisone on glycemic control in septic shock patients (n = 16). At baseline, a continuous 200 mg/daily hydrocortisone infusion was replaced by a single bolus of 50 mg of the drug. Blood glucose was monitored hourly, and insulin infusion was kept constant for 6 h. Thereafter, hydrocortisone and adjustment of blood glucose were resumed according to standard treatment. Mean blood glucose level at baseline was 7.1 mmol/l. After bolus injection of hydrocortisone, it increased within 6 h to peak levels of mean 8.6 mmol/l. Blood glucose returned to baseline with restoration of continuous hydrocortisone infusion. The data thus indicate that for glycemic control strategies, continuous infusions of hydrocortisone seem to be preferable to bolus injections.
Another potential confounder in glucose control is the route and continuity of nutrition. Nguyen et al. [59] assessed the relationship between blood glucose concentrations and intolerance to gastric feeding in critically ill patients in a case–control study. Two-hourly blood glucose levels and insulin requirements over the first 10 days after admission were assessed in 95 consecutive feed-intolerant (nasogastric aspirate > 250 ml during feed) critically ill patients and 50 age-matched, feed-tolerant patients who received feeds for at least 3 days. A standard insulin protocol was used to maintain glycemia at 5.0–7.9 mmol/l. The peak glucose levels were higher before and during enteral feeding in feed-intolerant patients. The mean and trough levels were, however, similar between the two groups on admission, at 24 h prior to feeding, and for the first 4 days of feeding. The variations over 24 h before and during enteral feeding were greater in feed-intolerant patients. A blood glucose level greater than 10 mmol/l was more prevalent in patients with feed intolerance. The time taken to develop feed intolerance was inversely related to glycemia at admission. Feed intolerance in critically ill patients is thus associated with a greater degree of glycemic variation and hyperglycemia. The data suggest that more intensive insulin therapy may be required to minimize feed intolerance. Obviously, deciding on cause–effect relationships in this matter is hard, since fluctuation of enteral feeding and absorption could easily affect glucose control as well.
Apart from confounding factors in glucose control, the manner by and degree to which tight glucose control is achieved is important. Meynaar et al. [60] evaluated the performance of a nurse-driven computerized insulin protocol in combination with bedside glucose measurements to maintain glycemia at 4.5–7.5 mmol/l, in a mixed adult ICU (n = 182 patients). Mean glucose decreased from 9.23 mmol/l prior to the protocol to 7.68 mmol/l with the final protocol aiming at glycemia of 4.5–7.5 mmol/l. Fifty-three percent of glucose measurements were within the target range, one episode of hypoglycemia (glucose ≤ 2.2 mmol/l) occurred, representing 0.5% of patients or 0.05% of glucose measurements. The combined strategy of successively more ambitious nurse-driven (computerized) insulin protocols and bedside glucose measurements resulted in acceptably low glucose levels with very few episodes of hypoglycemia. This is a careful study on how to achieve tighter control of glycemia with the help of the ICU nurses and point-of-care glucose measurements, preferably in arterial blood.
Lacherade et al. [61] studied failure to achieve glycemic control despite intensive insulin therapy in a medical ICU. The authors assessed the efficacy of an insulin treatment strategy in maintaining normoglycemia, and compared ICU mortality in patients (n = 105) who did and did not reach normoglycemia (≤ 7 mmol/l). Failure to control (mean glucose > 7 mmol/l after initial hyperglycemia correction) occurred in 32 patients and was associated with an increase in ICU mortality (56% vs. 23% in patients with successful control). In the multivariate analysis, failure to control independently predicted death in the ICU. This study emphasizes the difficulty of achieving tight control in complex medical cases but does not indicate whether failure is a marker or a mediator of a downhill course. The accuracy of bedside capillary blood glucose measurements in critically ill patients was the subject of study by Critchell et al. [62]. They compared the accuracy of fingerstick with laboratory venous plasma glucose measurements (laboratory glucose) in medical ICU patients (n = 80) and evaluated the factors involved (n = 277). Accuracy was defined as the percentage of paired values not in accord (> 0.83 mmol/l difference for laboratory values < 4.1 mmol/l and > 20% difference for laboratory values > / = 4.1 mmol/l). Outliers (blood glucose difference > 5.6 mmol/l) were excluded from analyses. Mean fingerstick glucose was 7.2 ± 2.5 mmol/l and mean laboratory glucose 6.8 ± 2.4 mmol/l. The correlation coefficient was 0.91 (Clinical and Laboratory Standards Institute threshold 0.9751). The mean difference (bias) between the two methods was 0.48 ± 1.0 mmol/l, and limits of agreement were +2.5 mmol/l and –1.6 mmol/l. Fifty-three paired measurements (19%) in 22 patients were not in accord. In 83% of these measurements, fingerstick was higher than laboratory glucose. The findings suggest that capillary blood glucose measured by fingerstick is inaccurate in critically ill patients, does not meet the CLSI standard and should be used with great caution in protocols of tight glycemic control. This type of study teaches us how to achieve tight glucose control at the bedside.
In addition to the increasing knowledge on the importance of glucose control, insight into the mechanisms, diagnosis and therapeutic consequences of adrenocortical dysfunction in the critically ill is expanding. The pituitary–adrenal response to human corticotropin-releasing hormone (hCRH) in critically ill, mechanically ventilated patients (n = 37) was studied by Dimopoulou et al. [63]. A morning blood sample was obtained to measure baseline cortisol, corticotropin (ACTH) and cytokines. Patients were then injected with 100 μg hCRH, and plasma cortisol and ACTH were measured over 2 h. Baseline and peak cortisol concentrations following hCRH were 16 ± 5 μg/dl and 21 ± 5 μg/dl, and median baseline and peak ACTH levels 23 pg/ml and 65 pg/ml, respectively. Higher ACTH levels and longer release of cortisol were noted in non-survivors (n = 18) than in survivors (n = 19). Furthermore, non-survivors had higher concentrations of interleukin 8 and 6 than survivors. Hence, critically ill patients with the highest degree of inflammatory profile who ultimately die particularly demonstrate altered pituitary–adrenal axis responses to hCRH. The data in fact argue in favor of activation of the hypothalamus–pituitary–adrenal axis as a function of disease severity and subsequent stress but do not indicate whether this response is sufficient or not.
RAI in patients with severe acute pancreatitis was studied by De Waele et al. [64]. This study aimed to analyze the incidence of RAI, to identify risk factors, and to describe how RAI affects outcome. In this prospective study, 25 patients with severe acute pancreatitis were subjected to a short 250 μg ACTH test within 5 days after admission. Median baseline cortisol level was 26.6 μg/dl and increased upon ACTH to 43.2 μg/dl and 48.8 μg/dl after 30 and 60 min, respectively. RAI (increment in cortisol < 9 μg/dl) was found in 16% of all patients and in 27% of patients with organ dysfunction. Patients with RAI were more severely ill and had higher SOFA scores. All patients with RAI developed pancreatic necrosis, and all of them needed surgical intervention. Twenty-eight-day mortality was higher in patients with RAI (75% vs. 5%). That increasing disease severity is a risk factor for RAI has been described before, but not the prognostic significance of RAI in this disease. Conversely, it is likely that RAI was particularly a manifestation of infected pancreatic necrosis with severe disease and dismal outcome.
The prognostic value of the adrenocortical response to ACTH was indeed further studied by Riché et al. [65] in 118 consecutive septic shock patients undergoing laparotomy or drainage for intra-abdominal infection. Baseline cortisol and delta cortisol to ACTH were measured during the first 24 h following onset of shock. Baseline levels were higher in non-survivors (39%) than in survivors, but the response to ACTH did not differ between outcome groups. Receiver operating characteristic curves showed threshold values for baseline of 32 μg/dl and a delta of 8  μg/dl that best discriminated survivors from non-survivors. There was no predictive value for hospital survival, however. Adrenal function tests and survival did not differ between patients who received etomidate anesthesia (n = 69) and those who did not. This study thus sheds doubts on the prognostic significance of ACTH test results in abdominal sepsis.
Dimopoulou et al. [66] report on a prospective study on adrenocortical responses and outcome prediction in 203 mixed ICU patients. Within 24 h of admission to the ICU a morning blood sample was obtained to measure baseline cortisol, ACTH, and dehydroepiandrosterone sulfate (DHEAS). Subsequently a low-dose (1 μg) ACTH test was performed. Overall, 149 patients survived and 54 died. Non-survivors were older and had higher SOFA and APACHE II scores. Non-survivors had a lesser rise in cortisol (5.0 vs. 8.3 μg/dl) and lower DHEAS (1065 vs. 1642 ng/ml) than survivors. The two groups had similar baseline and stimulated cortisol values. Multivariate logistic regression analysis revealed that the incremental rise in cortisol was an independent predictor for poor outcome. In contrast, baseline cortisol or adrenal androgens did not afford prognostic significance. This study underscores the prognostic significance of ACTH-stimulated cortisol but does not reveal whether the type of patient is of importance (sepsis vs. non-sepsis).
Finally, Venkatesh et al. [67] found evidence for altered cortisol metabolism in critically ill patients. Changes in cortisol metabolism may be due to altered activity of the enzyme 11beta-hydroxysteroid dehydrogenase (11beta-HSD). Patients with sepsis (n = 13), multiple trauma (n = 20), and burns (n = 19) were studied concerning serial plasma cortisol:cortisone ratios. Compared with controls, the plasma cortisol:cortisone ratio was elevated in sepsis and trauma on day 1 (22 ± 9, p = 0.01, and 23 ± 19, p = 0.0003, respectively) and remained elevated over the study period. Such a relationship was not demonstrable in burns. The ratio correlated to APACHE II (r = 0.77, p = 0.0008) and SAPS (r = 0.7, p = 0.003) only on day 7 and only in the burns cohort. Hence, there is evidence of altered cortisol metabolism in critical illness due to an increase in 11beta-HSD activity, but the precise relation with ACTH test results and clinical features of RAI remains unclear. Taken together, the optimal definition, clinical significance, risk factors, and mechanisms of adrenocortical suppression in critical illness are still a matter of debate and subject to further study.
