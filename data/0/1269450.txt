Equipment review: The success of early goal-directed therapy for septic shock prompts evaluation of current approaches for monitoring the adequacy of resuscitation

A recent trial utilizing central venous oxygen saturation (SCVO2) as a resuscitation marker in patients with sepsis has resulted in its inclusion in the Surviving Sepsis Campaign guidelines. We review the evidence behind SCVO2 and its relationship to previous trials of goal-directed therapy. We compare SCVO2 to other tools for assessing the adequacy of resuscitation including physical examination, biochemical markers, pulmonary artery catheterization, esophageal Doppler, pulse contour analysis, echocardiography, pulse pressure variation, and tissue capnometry. It is unlikely that any single technology can improve outcome if isolated from an organized pattern of early recognition, algorithmic resuscitation, and frequent reassessment. This article includes a response to the journal's Health Technology Assessment questionnaire by the manufacturer of the SCVO2 catheter.

In 2001, Rivers and coworkers [1] reported findings from a landmark investigation of early goal-directed therapy (EGDT) for septic shock. They hypothesized that current resuscitation strategies rely on inadequate indices of the adequacy of perfusion, and that resuscitation titrated to central venous oxygen saturation (SCVO2) would improve survival. In their trial, protocol-driven resuscitation of patients with systemic inflammatory response syndrome (SIRS) and a systolic blood pressure below 90 mmHg (after a 30 ml/kg fluid challenge) or a blood lactate concentration of 4 mmol/l or greater resulted in a hospital mortality rate of 30.5%, which was significantly less than the mortality rate (46.5%) in the cohort randomly assigned to usual care. As a result of this single-center randomized trial, the use of SCVO2 was given a grade B recommendation in the recent Surviving Sepsis Campaign recommendations [2].
The study's findings are compelling, but the universal adoption of the 'Rivers protocol' would require a departure from current practice in many institutions. The results from the study by Rivers and colleagues have stimulated debate in the fields of critical care and emergency medicine. One of the central questions in this debate is whether it is necessity to use measurements of SCVO2 to guide resuscitation. Is SCVO2 essential to the EGDT approach, or might other, alternative indices of the adequacy of resuscitation serve as well or better? In view of this debate, our aims here are to review briefly previous sepsis resuscitation studies and discuss factors that may have made EGDT successful as compared with previous attempts, and to examine other currently available markers of resuscitation.
Benjamin Wallace
Edwards Presep Central Venous Oximetry catheters measure oxygen concentration in venous blood via reflection spectrophotometry. Because deoxygenated hemoglobin and oxyhemoglobin absorb light differently at selected wavelengths, the reflected light can be analyzed to determine the percentage of SCVO2. This measurement is continuous and updates every 2 s.
The primary indication for the use of Presep is as a part of EGDT in the detection and treatment of patients with early sepsis. These patients may present in the emergency room, in the wards, or in the intensive care unit.
Mixed venous oxygen saturation (SVO2) has for some time been used as a guide to resuscitation adequacy in critically ill patients. Use of SCVO2, as a surrogate for SVO2, is indicated in critically ill patients requiring monitoring for resuscitation in whom placement of a pulmonary artery catheter is not warranted.
The accuracy of SVO2 measurement via spectrophotometry is very well documented and considered the 'gold standard'. Over the past 10 years many papers have been written to assess the usability of SCVO2 as a surrogate for SVO2 [3-5].
The majority of evidence to support the use of Presep SCVO2 catheters comes from the work of Rivers and coworkers [6], but there are also a number of optimization studies using forms of goal-directed therapy and SVO2 from groups such as that of Polonen [7] that show beneficial outcomes summarized in Dr Shoemakers' meta-analysis [8]. The Surviving Sepsis Campaign announced guidelines for the treatment of sepsis [9], including EGDT guided by SCVO2. Workers from St Georges Hospital have just presented an abstract at ESICM on SCVO2 monitoring in high-risk surgery [10].
To utilize the Edwards Presep SCVO2 catheter one requires hardware in the form of a module from a major patient monitor company or an Edwards oxygen saturation monitoring device (e.g. Vigilance, Explorer, SAT2, among others). In addition, one Edwards Presep oximetry catheter is required per patient.
The beauty of this minimally invasive technology is that it only requires the insertion of a standard central venous catheter using the Seldinger technique. A single calibration is required before insertion; subsequent calibrations can be perfomed in vivo through a simple venous blood sample. All patients eligible for central venous catheter placement can receive the benefits of this technology.
Since its release only 6 months ago in Europe and the USA, 2500 patients have received continuous SCVO2 monitoring; its sister parameter, SVO2, is continuously measured in more than 300,000 patients a year and has been available for 15 years.
EGDT using SCVO2 has proven efficacy in the emergency room. Studies are currently being conducted to supplement the work done in sepsis in the intensive care unit (ICU), with research settings including congestive heart failure, trauma and high-risk surgery.
Scott R Gunn and Mitchell P Fink
Clinical research in resuscitation end-points increased after Shoemaker and coworkers [11] reported that mortality was decreased when high-risk surgical patients were titrated to so-called supranormal values for cardiac index (≥ 4.5 l/min per m2) and oxygen delivery (≥ 600 ml/min per m2). However, two large multicentric randomized controlled trials (RCTs) conducted by Hayes [12] and Gattinoni [13] and their coworkers failed to corroborate the findings obtained by the Shoemaker group. Indeed, in the study by Hayes and colleagues [12], the mortality rate actually was significantly greater for patients randomly assigned to be resuscitated to supranormal indices than it was for patients assigned to usual care. Why did these RCTs not show a positive effect on mortality? It is important to examine carefully the factors that may contribute to the success or failure of a single center RCT that is discordant with the results of multicenter RCTs [14].
Factors that might influence outcome include resuscitation protocol, resuscitation end-points and the technologies used to measure those end-points, timing of interventions, and baseline mortality rate. Because of the bundled nature of care during resuscitation and the complex pathophysiology underlying sepsis, it may be difficult to identify any one single factor that strongly determines outcome.
It is difficult or impossible to achieve adequate blinding in trials designed to compare resuscitation strategies. To overcome the potential for bias introduced by the absence of blinding, investigators have stressed the importance of using protocolized care for both the intervention and control arms. In the trial conducted by Rivers and coworkers [1], control individuals were resuscitated using an algorithm that included the administration of fluid boluses titrated to increase central venous pressure (CVP) to 8–12 mmHg. Vasopressors were titrated to maintain mean arterial pressure between 65 and 90 mmHg. Maintaining urine output at 0.5 ml/kg per hour or greater was also a stated resuscitation target, although the strategies to be used to achieve this goal were not specified. Of individuals in the control group, 86% achieved these hemodynamic and urine output targets, whereas 99% of those in the intervention group achieved hemodynamic and SCVO2 goals [1].
Data obtained in a large multicentric RCT support the view that a liberal red cell transfusion policy designed to maintain hemoglobin concentration at 10 g/dl or greater may be deleterious in stable, critically ill patients [15]. However, the effects of red blood cell transfusion during the resuscitation of acutely ill, septic patients may be different. Both Hayes [12] and Gattinoni [13] and coworkers used the same threshold for transfusion (hemoglobin concentration <10 g/dl) as was used by Rivers and colleagues [1].
A recent meta-analysis [16] suggested that aggressive resuscitation efforts that begin early (before the onset of organ failure) may prove more beneficial than resuscitation carried out after the establishment of organ failure. Gattinoni and coworkers [13] enrolled patients in the SVO2 trial after 48 hours in the ICU, and Hayes and colleagues [12] enrolled patients upon arrival at the ICU, irrespective of the time from the presumed onset of critical illness. Thus, both of these (negative) studies enrolled patients who were already in an ICU. In contrast, Rivers and colleagues [1] enrolled patients on presentation to the emergency department. These patients had already received 6 hours of protocolized care before their arrival in the ICU. Thus, early identification and initiation of treatment may be a key element in the effectiveness of resuscitation interventions. Perhaps by actively enrolling individuals with cryptic sepsis or compensated shock, Rivers and colleagues intervened at a point before multisystem organ dysfunction had developed.
Rivers and coworkers [1] enrolled only patients with a suspected infection presenting to the emergency department with two out of four SIRS criteria and hypotension after a fluid bolus or an elevated blood lactate concentration. Thus, the study focused on patients with clear evidence of tissue hypoperfusion. In contrast, both the Hayes [12] and Gattinoni [13] studies enrolled a diverse population of critically ill patients at varying times throughout their illness.
In the intervention (experimental) arms of both the Hayes [12] and Gattinoni [13] trials, the goals for resuscitation were indices (cardiac index and systemic oxygen delivery) that are usually measured using a pulmonary artery catheter (PAC). In contrast, the end-point for resuscitation in the trial by Rivers and coworkers [1] was a parameter (SCVO2) that was measured by using an oximetric central venous catheter. Is it possible that the positive results in the study by Rivers and colleagues were simply due to the difference in the primary end-point for resuscitation? Although it is hard to be certain, this notion seems improbable to us. Recall that the trial performed by Gattinoni and coworkers [13] included a third arm in which patients were resuscitated to a SVO2 of greater than 70% (measured using a PAC). The patients in this arm of the study did not fair any better than those in the other two arms.
Adding an oximetric probe to a central venous catheter allows continuous measurement of SCVO2. Whether measurements of SCVO2 are a reasonable surrogate for measurements of SVO2 is controversial. Although the absolute values of SCVO2 are almost always higher than values for SVO2, Rivers and colleagues [17] maintain that the two parameters track one another closely over a range of hemodynamic states. This notion, however, was recently challenged [18]. The advantage of using an oximetric central venous catheter as compared with a PAC is that the former device can be inserted more rapidly and is less expensive. Placement of an oximetric central venous catheter also may be associated with fewer complications, although this idea has never been rigorously tested.
If we accept that the use of an oximetric central line was not the sole reason for success in the Rivers trial, then are there other resuscitative markers that may serve as adequate endpoints for an early goal-directed strategy? Many markers and technologies have promise in guiding resuscitation. However, currently, no one marker is perfect.
Physical examination and vital signs have traditionally been used to screen for hypovolemia and/or shock. However, heart rate, blood pressure, and capillary refill are not sensitive predictors of hypovolemia [19]. Moreover, these 'vital signs' do not correlate with other indicators of shock that require more invasive methods for measurement. In one study [20], 50% of critically ill patients presenting with shock and resuscitated to normal vital signs continued to have elevated blood lactate levels and low SCVO2.
Recently, there has been considerable interest in pulse pressure or systolic pressure variation as clinical predictors of preload responsiveness, but these signs predict the response to fluid loading – not whether fluid is likely to beneficial [21]. Nevertheless, it seems reasonable to hypothesize that optimizing preload will improve outcome, particularly if the strategy is instituted early in the course of illness (e.g. during the first 6 hours after presentation). Normalization of blood lactate level or base deficit might also be of value, but continuous monitoring of blood pH or lactate concentration is not currently available in most centers.
Circulating levels of procalcitonin [22], TREM-1 [23], molecules associated with induction of apoptosis (e.g. tumor necrosis factor and Fas ligand) [24], interleukin-6 [25], and other biochemical indices of inflammation have been proposed as diagnostic markers of sepsis and SIRS. These markers may be clinically useful in the initial diagnosis and stratification of sepsis [26], but these parameters are not useful for the minute-to-minute titration of care.
It is well established that a high blood lactate concentration portends a poor prognosis for patients with shock due to various causes [27-32]. Furthermore, a prompt decrease in circulating lactate level in response to resuscitation is a good prognostic indicator [33]. These considerations notwithstanding, measurements of blood lactate concentration are not useful for the titration of resuscitation because this biochemical parameter responds too slowly to changes in perfusion and, by itself, blood lactate concentration provides no information regarding key parameters such as preload responsiveness.
The goal of resuscitation is to ensure adequate oxygen delivery to tissues. At the level of the whole body, oxygen delivery is the product of cardiac output and arterial oxygen content. In septic shock, the predominant reason for low cardiac output is inadequate preload. CVP is often used as a surrogate for preload [34]. However, even direct measurements of left atrial pressure do not correlate perfectly with left ventricular end-diastolic volume (i.e. preload), because the relationship between intracavitary volume and pressure is inconsistent both among patients and in the same patient over time [35,36]. CVP, of course, is even less likely than left atrial pressure to reflect left ventricular end-diastolic volume accurately, because it is affected by right ventricular afterload (i.e. pulmonary arterial pressure) as well as right ventricular compliance.
For many years, the PAC was considered the 'gold standard' for monitoring systemic hemodynamic parameters, such as cardiac output, left ventricular preload, and systemic oxygen delivery. The PAC can be used to measure central venous and pulmonary artery pressures. Using thermodilution, this device permits repetitive or even continuous estimates of cardiac output. In addition, whether by using oximetric technology or blood gas analysis, the PAC permits measurements of systemic oxygen delivery, SVO2, and systemic oxygen extraction ratio.
In 1996, Connors and coworkers [37] reported surprising results in a major observational study evaluating the value of pulmonary artery catheterization in critically ill patients. Those investigators took advantage of an enormous data set that had previously been (and prospectively) collected for another purpose at five major teaching hospitals in the USA. They compared two groups of patients: those who did and those who did not undergo placement of a PAC during their first 24 hours of ICU care. They recognized that the value of their intended analysis was completely dependent on the robustness of their methodology for case matching, because sicker patients (i.e. those at greater risk for mortality based on the severity of their illness) were presumably more likely to undergo pulmonary artery catheterization. Accordingly, the authors used sophisticated statistical methods for generating a cohort of study (i.e. PAC) patients, each one having a paired control matched carefully for severity of illness. A critical assessment of their published findings supports the view that the cases and their controls were indeed well matched with respect to a large number of pertinent clinical parameters. Remarkably, Connors and coworkers concluded that placement of a PAC during the first 24 hours of stay in an ICU is associated with a significant increase in the risk for mortality, even when statistical methods are used to account for severity of illness.
Although the report by Connors and coworkers [37] generated an enormous amount of controversy in the medical community, the results reported actually confirmed the results of two prior similar observational studies. The first of these studies [38] used a database of 3263 patients with acute myocardial infarction treated in central Massachusetts in 1975, 1978, 1981, and 1984 as part of the Worcester Heart Attack Study. For all patients, hospital mortality was significantly greater for patients treated using a PAC, even when multivariate statistical methods were employed to control for key potential confounding factors. The second large observational study of patients with acute myocardial infarction [39] also found that hospital mortality was significantly greater for patients managed with the assistance of a PAC, even when the presence or absence of 'pump failure' was considered in the statistical analysis. In neither of these earlier reports did the authors conclude that placement of a PAC was truly the cause of worsened survival after myocardial infarction. As a result of the study by Connors and coworkers, experts in the field questioned the value of bedside pulmonary artery catheterization, and some even called for a moratorium on the use of the PAC [40].
Since publication of the 'Connors study' [37], a large observational study of patients admitted to a medical ICU [41] concluded that patients who underwent placement of a PAC were sicker than those who did not receive this type of monitoring. However, risk-adjusted mortality was similar for patients treated with and those treated without use of a PAC.
Relatively few prospective RCTs of pulmonary artery catheterization have been performed. All of these studies are flawed in one or more ways. The study by Pearson and coworkers [42] was very underpowered; only 226 patients were enrolled. In addition, the attending anesthesiologists were permitted to exclude patients from the CVP group at their discretion; thus, randomization was compromised. The study by Tuman and coworkers [43] was large (1094 patients were enrolled), but different anesthesiologists were assigned to the different groups. Furthermore, 39 patients in the CVP group underwent placement of PAC because of hemodynamic complications. All of the individual single-institution studies of vascular surgery patients were relatively underpowered [44-47], and all excluded at least certain categories of patients (e.g. those with a history of recent myocardial infarction).
In the largest RCT to evaluate the use of the PAC, Sandham and coworkers [48] randomly assigned 1994 high-risk patients undergoing major thoracic, abdominal, or orthopedic surgery to placement of a PAC or a CVP catheter. In the patients assigned to receive a PAC, physiologic goal-directed therapy was implemented by protocol. There were no differences at 30 days, 6 months, or 12 months in mortality between the two groups, and ICU length of stay was similar. There was a significantly higher rate of pulmonary emboli in the PAC group (0.9% versus 0%). This study has been criticized because most of the patients enrolled were not in the highest risk category.
The limitations of these studies notwithstanding, the weight of current evidence suggests that routine use of the PAC is not useful for the vast majority of patients undergoing cardiac, major peripheral vascular, or ablative surgical procedures. Whether use of a PAC for early goal-directed resuscitation of patients with sepsis would lead to results better or worse than those obtained by Rivers and coworkers [1] is not known. It is clear, however, that if routine placement of a central venous catheter would stress the capabilities of most emergency departments, then routine PAC placement would be even more problematic.
When ultrasonic sound waves are reflected by moving erythrocytes in the bloodstream, the frequency of the reflected signal is increased or decreased, depending on whether the cells are moving toward or away from the ultrasound source. This change in frequency is called the Doppler shift, and its magnitude is determined by the velocity of the moving red blood cells. Thus, measurements of Doppler shift can be used to calculate red blood cell velocity. With knowledge of both the cross-sectional area of a vessel and the mean red blood cell velocity of the blood flowing through it, one can calculate blood flow rate. If the vessel in question is the aorta, then cardiac output can be calculated.
Two approaches have been developed for using Doppler ultrasonography to estimate cardiac output. The first approach uses an ultrasonic transducer, which is manually positioned in the suprasternal notch and focused on the root of the aorta. Aortic cross-sectional area can be estimated using a nomogram (that factors in age, height, and weight), back calculated if an independent measure of cardiac output is available, or by using two-dimensional transthoracic or transesophageal ultrasonography. Although this approach is completely noninvasive, it requires a highly skilled operator in order to obtain meaningful results and is quite labor intensive. Moreover, unless cardiac output measured using thermodilution is used to back calculate aortic diameter, accuracy using the suprasternal notch approach is not acceptable [49,50]. Accordingly, the method is useful only for obtaining very intermittent estimates of cardiac output and has not been widely adopted by clinicians.
Another more promising approach was originally introduced by Daigle and colleagues [51]. In this method, blood flow velocity is continuously monitored in the descending thoracic aorta using a transducer introduced into the esophagus. The current embodiment of this concept, called the CardioQ, is manufactured by Deltex Medical Limited (Chichester, UK). The device consists of a continuous wave Doppler transducer mounted at the tip of a transesophageal probe. In order to maximize the accuracy of the device, the probe position must be adjusted to obtain the peak velocity in the aorta. To transform blood flow in the descending aorta into cardiac output, a correction factor is applied, which is based on the assumption that only a portion of the flow at the root of the aorta is still present in the descending thoracic aorta. Aortic cross-sectional area is estimated using a nomogram based on the patient's age, weight, and height. Results using these methods appeared to be sufficiently accurate across a broad spectrum of patients to be clinically useful [52]. In that multicenter study, good correlation was found between cardiac output measurements obtained using the esophageal Doppler method and thermodilution. The ultrasound device also calculates a derived parameter, called flow time corrected (FTc), which is the systolic flow time in the descending aorta corrected for heart rate. FTc is a function of preload, contractility, and vascular input impedance. Although it is not a pure measure of preload, Doppler-based estimates of stroke volume and FTc have been used successfully to guide volume resuscitation in high-risk surgical patients undergoing major operations [53]. Indeed, evidence is accumulating that esophageal Doppler monitoring coupled with algorithmic resuscitation improves outcomes in surgical patients [54,55].
The impedance to flow of alternating electrical current in regions of the body is commonly called 'bioimpedance'. In the thorax, changes in the volume and velocity of blood in the thoracic aorta lead to detectable changes in bioimpedance. The first derivative of the oscillating component of thoracic bioimpedance (dZ/dt) is linearly related to aortic blood flow. On the basis of this relationship, empirically derived formulas have been developed to estimate stroke volume (and hence cardiac output) noninvasively. This methodology is called impedance cardiography. The approach is attractive because it is completely noninvasive, provides a continuous read-out of cardiac output, and does not require extensive training for use. Despite these advantages, a number of studies suggest that measurements of cardiac output obtained by impedance cardiography are not sufficiently reliable to be used for clinical decision making and have poor correlation with standard methods such as thermodilution and ventricular angiography [56-58].
Perhaps one of the least invasive and most appealing methods for determining cardiac output uses an approach called pulse contour analysis, originally described by Wesseling and colleagues [59] for estimating stroke volume on a beat-to-beat basis. The mechanical properties of the arterial tree and stroke volume determine the shape of the arterial pulse waveform. The pulse contour method of estimating cardiac output uses the arterial pressure waveform as an input in a model of the systemic circulation in order to determine beat-to-beat stroke volume. The parameters of resistance, compliance, and impedance are initially estimated based on the patient's age and sex, and can be subsequently refined by using a reference standard measurement of cardiac output. A commercially available device, the PulseCO Hemodynamic Monitor (LiDCO, Ltd, London, UK) utilizes this technology. In the LiDCO system, the reference standard estimation of cardiac output is obtained periodically using the indicator dilution approach by injecting the indicator (a dilute solution of lithium ion) into a central venous catheter and detecting the transient increase in lithium ion concentration in the blood using an arterial catheter equipped with a lithium-sensitive sensor in a femoral artery.
Measurements of cardiac output based on pulse contour monitoring are comparable in accuracy to standard PAC thermodilution methods, but they use an approach that is much less invasive because arterial and central venous, but not transcardiac, catheterization is needed [60]. Using online pressure waveform analysis, the computerized algorithms can calculate stroke volume, cardiac output, systemic vascular resistance, and an estimate of myocardial contractility, namely the rate of rise in arterial systolic pressure (dP/dT). One weakness of the pulse contour approach is that it does not directly provide an assessment of preload responsiveness. However, this information can be obtained by analyzing changes in pulse pressure over the respiratory cycle in mechanically ventilated patients (see below).
The use of pulse contour analysis has been applied using an even less invasive technology based on totally noninvasive photoplethysmographic measurements of arterial pressure [61]. However, the accuracy of this technique has been questioned [62] and its clinical utility remains to be determined.
Partial carbon dioxide rebreathing uses the Fick principle to estimate cardiac output noninvasively. By intermittently altering the dead space within the ventilator circuit via a rebreathing valve, changes in carbon dioxide production (VCO2) and end-tidal carbon dioxide (ETCO2) are used to determine cardiac output using a modified Fick equation (cardiac output = VΔCO2/ΔETCO2) [63]. A commercially available device, the NICO monitor (Novametrix Medical Systems, Inc., Wallingford, CT, USA) uses this Fick principle to calculate cardiac output using intermittent partial carbon dioxide rebreathing through a disposable rebreathing loop. The device consists of a carbon dioxide sensor based on infrared light absorption, an airflow sensor, and a pulse oximeter. Changes in intrapulmonary shunt and hemodynamic instability impair the accuracy of cardiac output estimated by partial carbon dioxide rebreathing. Continuous in-line pulse oximetry and fractional inspired oxygen are used to estimate shunt fraction to correct the cardiac output value obtained.
Some studies of the partial carbon dioxide rebreathing approach suggest that the accuracy of the NICO system is not good when thermodilution is used as the gold standard for measuring cardiac output [64,65]. However, other studies suggest that the partial carbon dioxide rebreathing method for determination of cardiac output compares favorably with measurements made using a PAC in critically ill patients [66]. Like some of the other minimally invasive methods discussed above, the NICO system does not provide a direct assessment of preload responsiveness.
Transesophageal echocardiography (TEE) has made the transition from operating room to the ICU. TEE requires that the patient be sedated. Using this powerful technology, global assessments of left and right ventricular function can be conducted, including determinations of ventricular volume, ejection fraction, and cardiac output. Segmental wall motion abnormalities, pericardial effusions, and tamponade can readily be identified with TEE. Doppler techniques allow estimation of atrial filling pressures. The technique is somewhat cumbersome and requires considerable training and skill in order to obtain reliable results.
When intrathoracic pressure increases during the application of positive airway pressure in mechanically ventilated patients, venous return decreases and, as a consequence, stroke volume also decreases. Therefore, pulse pressure variation (PPV) during a positive pressure breath can be used to predict the responsiveness of cardiac output to changes in preload [67]. PPV is defined as the difference between the maximal pulse pressure and the minimum pulse pressure divided by the average of these two pressures [67]. Michard and colleagues [67] validated this approach by comparing PPV, CVP, pulmonary artery occlusion pressure, and systolic pressure variation as predictors of preload responsiveness in a cohort of critically ill patients. They classified patients as being preload responsive if their cardiac index increased by at least 15% after rapid infusion of a standard volume of intravenous fluid. Receiver operating characteristic curves demonstrated that PPV was the best predictor of preload responsiveness. Although atrial arrhythmias can interfere with the usefulness of this technique [21], PPV remains a very useful approach for assessing preload responsiveness in most patients because of its simplicity and reliability.
Global indices of cardiac output or systemic oxygen delivery provide little useful information regarding the adequacy of cellular oxygenation and mitochondrial function. On theoretical grounds, measuring tissue pH to assess the adequacy of perfusion is an extremely attractive concept. As a consequence of the stoichiometry of the reactions responsible for the substrate level phosphorylation of ADP to form ATP, anaerobiosis is inevitably associated with the net accumulation of protons [68]. Accordingly, knowing that tissue pH is not in the acid range should be enough information to conclude that global perfusion (and, for that matter, arterial oxygen content) are sufficient to meet the metabolic demands of the cells, even without knowledge of the actual values for tissue blood flow or oxygen delivery. By the same token, the detection of tissue acidosis should alert the clinician to the possibility that perfusion is inadequate. Prompted by this reasoning, Fiddian-Green and colleagues [65,69,70] promulgated the idea that tonometric measurements of tissue partial carbon dioxide tension (PCO2) in the stomach or sigmoid colon could be used to estimate mucosal pH ('pHi') and thereby monitor visceral perfusion in critically ill patients.
Unfortunately, the notion of using tonometric estimates of gastrointestinal mucosal pH for monitoring perfusion is predicated on a number of assumptions, some of which may be partially or completely invalid. Furthermore, currently available methods for performing measurements of gastric mucosal PCO2 in the clinical setting remain rather cumbersome and expensive. Perhaps for these reasons, gastric tonometry for monitoring critically ill patients has never really caught on except as a research tool. Some recent developments in the field may be changing this situation, however, and monitoring tissue PCO2 may become a practical means for assessing the adequacy of perfusion.
Although PCO2 and pH are affected by changes in perfusion in all tissues, efforts to monitor these parameters in patients using tonometric methods have focused on the mucosa of the gastrointestinal tract, particularly the stomach, for both practical and theoretical reasons. From a practical standpoint, the stomach is already commonly intubated in clinical practice for purposes of decompression and drainage or feeding. Placement of a nasogastric or orogastric tube is generally regarded as minimally invasive. In addition, when global perfusion is compromised, blood flow to the splanchnic viscera decreases to a greater extent than does perfusion to the body as a whole [71]. Thus, a marker of compromised splanchnic perfusion should be a 'leading indicator' of impending adverse changes in blood flow to other organs [72]. Second, the gut has been hypothesized to be the 'motor' of the multiple organ system dysfunction syndrome [73], and in experimental models intestinal mucosal acidosis, whether due to inadequate perfusion or other causes, has been associated with hyperpermeability to hydrophilic solutes [74,75]. Therefore, ensuring adequate splanchnic perfusion might be expected to minimize derangements in gut barrier function and, on this basis, improve outcome for patients.
The stomach, however, may not be an ideal location for monitoring tissue PCO2. First, carbon dioxide can be formed in the lumen of the stomach when hydrogen ions secreted by parietal cells in the mucosa titrate luminal bicarbonate anions, which are present either as a result of backwash of duodenal secretions or secretion by gastric mucosal cells. Thus, measurements of gastric PCO2 and pHi can be confounded by gastric acid secretion, as documented in a study of normal volunteers by Heard and coworkers [76] and subsequently confirmed by others [77-79]. Consequently, accurate measurements of gastric PCO2 and pHi depend on pharmacologic blockade of luminal proton secretion using histamine receptor antagonists or proton pump inhibitors. The need to use pharmacologic therapy adds to the cost and complexity of the monitoring strategy. Second, enteral feeding can interfere with measurements of gastric mucosal PCO2, necessitating temporary cessation of the administration of nutritional support or the placement of a postpyloric tube [80].
Despite the problems noted above, measurements of gastric pHi and/or mucosal–arterial PCO2 gap have been shown to be good predictors of outcome in a wide variety of critically ill individuals, including general medical ICU patients [81,82], victims of multiple trauma [83-85], patients with sepsis [86], and patients undergoing major surgical procedures [70,87]. In studies using endoscopic measurements of gastric mucosal blood flow by laser Doppler flowmetry, the development of gastric mucosal acidosis has been shown to correlate with mucosal hypoperfusion [88]. The development of low pHi in the colon has been shown to correlate with an exaggerated host inflammatory response in patients undergoing aortic surgery [73]. Moreover, in a landmark prospective multicentric RCT of monitoring in medical ICU patients, titrating resuscitation to a gastric pHi end-point rather than conventional hemodynamic indices resulted in higher 30-day survival [89]. In another study, Ivatury and coworkers [90] randomly assigned 57 trauma patients to two groups. In the first group, administration of fluids and vasoactive drugs was titrated to achieve a gastric pHi greater than 7.30. In the second group, resuscitation was titrated to achieve a calculated systemic oxygen delivery index greater than 600 ml/min per m2 or systemic oxygen utilization greater than 150 ml/min per m2. Although survival was not significantly different in the two arms of the study, failure to normalize gastric pHi within 24 hours was associated with a very high mortality rate (54%), whereas normalization of pHi was associated with a significantly lower mortality rate (7%).
It seems likely that monitoring tissue PCO2 ('tissue capnometry') will play an increasingly important role in the management of critically ill patients because of two important insights. First, the directly measured parameter, namely tissue PCO2, provides more reliable information about perfusion than does the derived parameter pHi [91-93]. By eliminating the potentially confounding effects of systemic hypocapnia or hypercapnia, monitoring the gap between tissue PCO2 and arterial carbon dioxide tension (PaCO2) may prove to be even more valuable than simply following changes in tissue PCO2. The second recent insight is that it may not be necessary, or even desirable, to monitor tissue PCO2 in the stomach or other portions of gastrointestinal tract. For example, Sato and coworkers [94] showed that changes in gastric wall and esophageal tissue PCO2 track each other very closely in rats subjected to hemorrhagic shock. Similar results were reported by Guzman and coworkers [95] in a study of dogs infused with lipopolysaccharide. It appears probable that monitoring tissue PCO2 in other nongastric sites, such as the space under the tongue, may require even less invasion of the patient and yet be as informative as measuring PCO2 in the wall of the esophagus or the gut [96,97].
Results from preliminary clinical studies support the view that monitoring tissue PCO2 in the sublingual mucosa may provide valuable clinical information. Increased sublingual carbon dioxide tension (PslCO2) was associated with a decrease in arterial blood pressure and cardiac output in patients with shock due to hemorrhage or sepsis [98]. In a study of critically ill patients with septic or cardiogenic shock, the PslCO2–PaCO2 gradient was found to be a good prognostic indicator [99]. This study also demonstrated that sublingual capnography was superior to gastric tonometry in predicting patient survival. The PslCO2–PaCO2 gradient also correlated with the mixed venous–arterial PCO2 gradient, but it failed to correlate with blood lactate level, SVO2, or systemic oxygen delivery. These latter findings suggest that the PslCO2–PaCO2 gradient may be a better marker of tissue dysoxia than are these other parameters. Similar findings were reported in another study conducted by Marik and Bankov [100]. A device for sublingual capnometry, the CapnoProbe (Tyco Healthcare/Nellcor, Pleasonton, CA, USA), was being marketed commercially. The device was voluntarily recalled by the manufacturer in 2004, however, following hospital reports showing that Burkholderia cepacia, a pathogenic bacterium, could be cultured both from samples from patients monitored with CapnoProbes and from unused CapnoProbe sensors. A variety of other potentially pathogenic bacteria species were cultured from unused CapnoProbe sensors.
It is unlikely that placement of an oximetric central venous catheter alone without integration into an organized program of early recognition, algorithmic resuscitation, and frequent reassessment would improve sepsis mortality. Current levels of evidence do not support the full-scale adoption of SCVO2 monitoring to the exclusion of all other modalities. Sorting out what parts of the 'Rivers resuscitation package' affect sepsis mortality remains an important task. Use of the SCVO2 catheter (or any other monitoring strategy) without an organized program of care will be unsuccessful.
The best level of evidence that currently exists is from a single-center RCT. There are two, equally valid but mutually exclusive options: remain skeptical of adopting a new technology and treatment strategy that requires a major revision in the current standard of care until further data from a multicentric RCT are available; or adopt the technology (and protocol) with an attitude of cautious optimism and await the results of a definitive multicenter RCT. With either option, further research is imperative to address these questions. Can the results obtained by Rivers and coworkers [1] be replicated in a multicentric trial? What components of the 'Rivers protocol' are essential (e.g. is aggressive blood transfusion really needed)? Can the results obtained by Rivers and cowokers [1] be replicated even when less invasive forms of monitoring (e.g. esophageal Doppler-based estimates of cardiac output and preload responsiveness) are substituted for measurements of ScvO2? What is the biochemical basis for the apparent success of the 'Rivers protocol?'
CVP = central venous pressure; EGDT = early goal-directed therapy; FTc = flow time corrected; ICU = intensive care unit; PAC = pulmonary artery catheter; PCO2 = partial carbon dioxide tension; PaCO2 = arterial carbon dioxide tension; pHi = mucosal pH; PslCO2 = sublingual carbon dioxide tension; PPV = pulse pressure variation; RCT = randomized controlled trial; SCVO2 = central venous oxygen saturation; SVO2 = mixed venous oxygen saturation; SIRS = systemic inflammatory response syndrome; TEE = transesophageal echocardiography.
The author(s) declare that they have no competing interests.
